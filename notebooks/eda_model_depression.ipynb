{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "# Abschlussprojekt: Gebrauchtwagenkäufe (Lösung)\nModul 3 | Kapitel 4 | Notebook 1\n\nDu hast in den letzten Kapiteln und Modulen zahlreiche Data-Science-Methoden kennengelernt. Nachdem du im Übungsprojekt eine Regression auf Daten mit relativ wenigen Features durchgeführt hast, kannst du dich nun im Abschlussprojekt der Klassifizierung widmen. Dazu erhältst du ein Datenset, welches Gebrauchtwagenkäufe und unausgeglichene Zielkategorien beinhaltet.\n\n***\nAm Ende der Übung hast du:\n* Daten eingelesen und bereinigt,\n* Feature Engineering betrieben,\n* ein Modell an die Daten angepasst,\n* die einzelnen Schritte zu einer Datenpipeline kombiniert,\n* dein Modell interpretiert.\n***",
    "selectable": false,
    "tags": []
   },
   "source": [
    "# Portfolioprojekt: Depression and anxiety data  (EDA-file)\n",
    "\n",
    "The project was given from:\n",
    "\n",
    "https://www.kaggle.com/datasets/shahzadahmad0402/depression-and-anxiety-data/code\n",
    "\n",
    "\n",
    "***\n",
    "We have the following tasks:\n",
    "* Data reading and cleaning,\n",
    "* first Feature Engineering,\n",
    "* Base model, fit the dates to the model,\n",
    "* Combination to the pipeline,\n",
    "* Model interpretation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-set is given with th columns\n",
    "\n",
    "| **Column** | **Description** |\n",
    "| ------------ | :-----------------: |\n",
    "| id | each number is a participant in the experiment |\n",
    "| school_year | years in school |\n",
    "| age | |\n",
    "| gender | |\n",
    "| bmi | body mass index |\n",
    "| who_bmi | bmi category |\n",
    "| phq_score | measure the severity of symptoms related to depression, anxiety, and other related disorders in patients |\n",
    "| depression_severity | degree or intensity of symptoms experienced by an individual with depression |\n",
    "| depressiveness | |\n",
    "| suicidal | the candidate have suicide thought |\n",
    "| depression_diagnosis | the candidate already have depression diagnosis |\n",
    "| depression_treatment | the candidate already have depression treatment |\n",
    "| gad_score | measure that assesses the severity of Generalized Anxiety Disorder |\n",
    "| anxiety_severity |  intensity of symptoms experienced by an individual with anxiety |\n",
    "| anxiousness | |\n",
    "| anxiety_diagnosis | the candidate already have anxiety diagnosis |\n",
    "| anxiety_treatment | the candidate already have anxiety treatment |\n",
    "| epworth_score |  score to assess daytime sleepiness ytime sleepiness |\n",
    "| sleepiness | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA in Power-BI:\n",
    "We saw, that the column: pdh_score is very strong related to depressiveness (means: score >= 9 is depressive, score < 9 no depressive), further the depression_severity is also strong related to the depressiveness, such that we skip this 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "messageType": "szenario",
    "original-content": "**Szenario:** Du arbeitest als Data Scientist für einen US-amerikanischen Gebrauchtwagenhändler. Der Händler kauft gebrauchte Autos günstig in Onlineauktionen und von anderen Autoverkäufern, um sie dann auf der eigenen Plattform gewinnbringend weiterzuverkaufen. Es ist nicht immer einfach zu erkennen, ob sich der Kauf eines Gebrauchtwagens lohnt: Eine der größten Herausforderungen bei Gebrauchtwagenauktionen ist das Risiko, dass ein Auto solch schwerwiegende Probleme hat, die verhindern, dass es an Kunden weiterverkauft werden kann. Hierbei handelt es sich um sogenannte \"Montagsautos\" - also Autos, die von Hause aus erhebliche Mängel aufgrund von Produktionsfehlern aufweisen, welche die Sicherheit, die Verwendung oder den Wert dieses Autos erheblich beeinträchtigen und gleichzeitig nicht in einer angemessenen Anzahl an Reparaturen oder innerhalb eines bestimmten Zeitraums behoben werden können. Der Kunde hat in diesem Fall das Recht, sich den Kaufpreis zurückerstatten zu lassen. Neben den Anschaffungskosten führt der Fehleinkauf von solchen \"Montagsautos\" also zu erheblichen Folgekosten, wie z.B. der Einlagerung und Reparatur des Wagens, welche in Verlusten beim Weiterverkauf des Fahrzeugs resultieren können.\n\nDeshalb ist es deiner Chefin wichtig, möglichst viele Fehlkäufe von \"Montagsautos\" auszuschließen. Um die Einkäufer im Unternehmen bei der riesigen Anzahl an Angeboten zu entlasten, sollst du ein Modell entwickeln, welches vorhersagt, ob ein Angebot ein Fehlkauf im Sinne eines Montagsautos wäre. Das darf allerdings nicht dazu führen, dass zu viele gute Käufe ausgeschlossen werden. Genauere Angaben zu den Kosten und Gewinnen der jeweiligen Käufe erhältst du für die Entwicklung des Prototyps noch nicht.",
    "selectable": false
   },
   "source": [
    "**Szenario:** \n",
    "Based on the target 'depressivenes', we apply the EDA and a first base-model with logging and grid-search.\n",
    "\n",
    "We deal with a given csv-file and split it into a train + test dataset to apply the fit and prediction.\n",
    "Further, we have a data-cleaning, which ste the correct types and cleaned with NaN entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Preparation",
    "selectable": false
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Ein Data-Science-Modell hat immer die Aufgabe, ein bestimmtes Problem zu lösen. Also befasse dich am besten kurz nochmal mit der Aufgabenstellung und überlege dir den Kontext deines Modells.\n* Welches Problem soll das Modell lösen?\n* Welcher Art ist das Problem (z.B. Klassifikation, Regression, Clustering ...)\n* Wie sähe eine Anwendung aus, die dein Modell benutzt?\n* Welche Anforderungen stellt der Auftraggeber an dein Modell?\n* Welche Daten benötigst du, um dein Modell zu bauen?",
    "selectable": false
   },
   "source": [
    "We solve the following problem: Prediction of depressiveness given with important featurs: This is done by\n",
    "* Preparation of the dataset\n",
    "* Problem is based as a classification problem, which can be done with logistic regression, random forest, KNN, decision-tree\n",
    "* We apply the model to an aim-data set and predit, if a person has a tendency to depression or not\n",
    "* We like to built an app for a clinical application or an application for a Universities to see, if their students have some tendencises\n",
    "* We apply given datasets of a clinical study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Define Metric",
    "selectable": false
   },
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Anhand deines Verständnisses des vorliegenden Problems solltet du dir nun überlegen, welche Metrik(en) am besten geeignet sind, den Erfolg deines Modells zu beurteilen.",
    "selectable": false
   },
   "source": [
    "We apply the F1-matric to get a balance between the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "\n",
    "# Metrics considerations\n",
    "\n",
    "# Accuracy: The proportion of correct predictions. \n",
    "            # In your case this metric might be less important, \n",
    "           # I may have an unbalanced dataset (more good purchases than bad purchases).\n",
    "# Better metrics\n",
    "# Recall (Sensitivity): The proportion of correctly identified bad purchases out of all \n",
    "# actual bad purchases. Here I optimize: Identify as many bad purchases as possible.\n",
    "# Precision: The proportion of correctly identified bad purchases out of all as bad purchases \n",
    "# predicted cars. Important to minimize false purchases.\n",
    "# It is best to weight between the two:\n",
    "# F1-Score: The harmonic mean of precision and recall. \n",
    "# Here I have to find a balance between precision and recall.\n",
    "\n",
    "# We apply the F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Gather Data",
    "selectable": false
   },
   "source": [
    "### Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Die Daten befinden sich in der Datei *data_train.csv*. Der Zielvektor ist durch die Spalte `'IsBadBuy'` gegeben. Importiere die Module, die du typischerweise für das Einlesen und die Exploration benötigst und lies die Daten anschließend ein. ",
    "selectable": true
   },
   "source": [
    "We have the dataset: *depression_anxiety_data.csv*. With the target-column: 'depressiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Importiere z.B. die Module `pandas`, `numpy`, `seaborn` und `matplotlib.pyplot`.",
    "hint_counter": 0,
    "original-content": "# import modules \n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# import modules \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Columntransormer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# OneHotEndode,, Scaler\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Train/Test Splitt, Crossvaue-Score, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# read data\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# read data (start from the dataset)\n",
    "df = pd.read_csv(\"data/depression_anxiety_data.csv\")\n",
    "\n",
    "display(df.head())\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## EDA",
    "selectable": false
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Understand Data",
    "selectable": false
   },
   "source": [
    "### Understand Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Mach dich zu Beginn mit den Daten vertraut, damit du später bei der Datenbereinigung und -aufbereitung weißt, worauf du achten solltest.\n\n**Tipp**: Die Spalte `'PurchDate'` ist als Unix-Timestamp in ganzen Sekunden angegeben. Um diese Spalte mit `pandas` in ein Datumsformat umzuwandeln, kannst du folgenden Code nutzen: `my_df.loc[:, 'PurchDate'] = pd.to_datetime(my_df.loc[:, 'PurchDate'], unit='s')`",
    "selectable": true
   },
   "source": [
    "Mach dich zu Beginn mit den Daten vertraut, damit du später bei der Datenbereinigung und -aufbereitung weißt, worauf du achten solltest.\n",
    "\n",
    "**Tipp**: Die Spalte `'PurchDate'` ist als Unix-Timestamp in ganzen Sekunden angegeben. Um diese Spalte mit `pandas` in ein Datumsformat umzuwandeln, kannst du folgenden Code nutzen: `my_df.loc[:, 'PurchDate'] = pd.to_datetime(my_df.loc[:, 'PurchDate'], unit='s')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Bei mir sind nur ca. 12,3% der Datenpunkte ein vermeidbarer Kauf.",
    "hint_counter": 1,
    "original-content": "",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. See the datatypes\n",
    "print(\"\\nDatatypes of the column:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# 2. Statistical description\n",
    "\n",
    "print(\"\\n Statistical Description:\")\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check NaNs and duplicates\n",
    "print('Index')\n",
    "print('index_size', df.index.size)\n",
    "print('Columns with NaN')\n",
    "print('is NaN', df.isna().sum())\n",
    "print('Duplicates in Columns')\n",
    "print('duplicated', df.duplicated().sum())\n",
    "#note: no NaNs, no duplicates, no cleaning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exists duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "print(\"Show duplicates:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Clean duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "\n",
    "# Optional: Do we have duplicates\n",
    "print(\"Overview, if duplicates after cleaning:\")\n",
    "print('duplicated', df_clean.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the important columns\n",
    "columns_of_interest = [\n",
    "    'gender', 'who_bmi', 'depression_severity', 'depressiveness', 'suicidal', \n",
    "    'depression_diagnosis', 'depression_treatment', 'anxiety_severity', \n",
    "    'anxiousness', 'anxiety_diagnosis', 'anxiety_treatment', 'epworth_score', \n",
    "    'sleepiness'\n",
    "]\n",
    "\n",
    "# Show the unique values of each column\n",
    "for column in columns_of_interest:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique columns '{column}':\")\n",
    "    print(unique_values)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modells with targets and features\n",
    "\n",
    "# Target columns (later we put 'depression_treatment', 'anxiety_treatment' together to 'treatment_status')\n",
    "\n",
    "target_cols =['anxiousness', 'depressiveness', 'depression_treatment', 'anxiety_treatment', 'suicidal']\n",
    "\n",
    "# first all numerical columns\n",
    "\n",
    "num_cols =  ['school_year', 'age', 'bmi', 'phq_score', 'gad_score', 'epworth_score' ]\n",
    "\n",
    "\n",
    "# categorical columns, which can be transformed simple to numerical columns (only true/false entires)\n",
    "\n",
    "cat_cols_trans = ['gender', 'depression_diagnosis', 'depression_treatment',  'anxiety_diagnosis', 'sleepiness']\n",
    "\n",
    "\n",
    "# We have 3 catergoical columns with more that 2 entries:\n",
    "\n",
    "cat_cols = ['who_bmi', 'anxiety_severity', 'depressiv_severity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Datatypes:\n",
    "\n",
    "\n",
    "# Drop all NaNs (we have ony a few NaNs in the columns): \n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# Correct Datatypes of the target:\n",
    "# and the feature gender (both int)\n",
    "\n",
    "df.gender = df.gender.map({'male':1, 'female':0})\n",
    "\n",
    "\n",
    "# Define the targets:\n",
    "#1.) anxiousness:  Prediction:  0: False (no anxious) / 1: True (anxious)\n",
    "#2.) depressiveness: Prediction: 0: False (no depression) / 1: True (depression)\n",
    "#3.) will_get_treatment: Prediction: 0: False (no get-treatement) / 1: True (get-treatment)\n",
    "#4.) suicidality: Prediction: 0: False (no suicidality) / 1: True (suicidality)\n",
    "\n",
    "df['anxiousness'] = df['anxiousness'].astype(int)\n",
    "df['depressiveness'] = df['depressiveness'].astype(int)\n",
    "\n",
    "df['treatment_status'] = df['depression_treatment'] | df['anxiety_treatment']\n",
    "df['treatment_status'] = df['treatment_status'].astype(int)\n",
    "\n",
    "\n",
    "df['suicidal'] = df['suicidal'].astype(int)\n",
    "\n",
    "# Change the binary categorial feature variables into numerical \n",
    "\n",
    "# Convert binary categorical features to integer\n",
    "\n",
    "# List of categorical columns with binary values that need to be converted to integers\n",
    "cat_cols_trans = ['gender', 'depression_diagnosis', 'depression_treatment',  \n",
    "                      'anxiety_diagnosis', 'sleepiness']\n",
    "\n",
    "df[cat_cols_trans] = df[cat_cols_trans].astype(int)\n",
    "\n",
    "\n",
    "df['anxiety_treatment'] = df['anxiety_treatment'].astype(int)\n",
    "\n",
    "# Describe the dates\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Durch jede Spalte iterieren und die Verteilung der Werte anzeigen\n",
    "for column in columns_of_interest:\n",
    "    value_counts = df[column].value_counts()\n",
    "    print(f\"Verteilung der Werte in der Spalte '{column}':\")\n",
    "    print(value_counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and correctionof the types\n",
    "\n",
    "print(\"\\nDatentypen der Spalten:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of values in each feature\n",
    "# Solution:\n",
    "display(df.describe())\n",
    "display(sns.pairplot(df))  # needs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# critical features, we skip to predict the 'depressiveness':   'phq_score', 'depression_severity'\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "# (depression_treatment and anxiety_treatment  are in the target treatment_status)\n",
    "\n",
    "cat_cols = ['who_bmi', 'anxiety_severity']\n",
    "\n",
    "# Selected columns\n",
    "selected_columns = num_cols\n",
    "\n",
    "# Korrelationsmatrix der ausgewählten Spalten\n",
    "corr_matrix = df[selected_columns].corr()\n",
    "\n",
    "\n",
    "# Heatmap der Korrelationsmatrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin = -1, vmax = 1, fmt='.2f')\n",
    "plt.title('Korrelationsmatrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "\n",
    "num_cols__no_pca = ['gender', 'bmi']\n",
    "\n",
    "num_cols_pca = ['school_year', 'age', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "# (depression_treatment and anxiety_treatment  are in the target treatment_status)\n",
    "\n",
    "cat_cols = ['who_bmi', 'anxiety_severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram and boxplot for the numerical features with the target \"depressiveness\" as Hue\n",
    "\n",
    "for col in num_cols:\n",
    "\n",
    "    # Full plot histogram and boxplot\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 6), sharex=True, gridspec_kw={'height_ratios': [5, 1]})\n",
    "    \n",
    "    # Histogram\n",
    "    sns.histplot(data=df_clean, x=col, hue='depressiveness', kde=True, multiple=\"stack\", ax=axes[0])\n",
    "    axes[0].set_title(f'Histogram of {col} by depressiveness')\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df_clean, x=col, hue='depressiveness', ax=axes[1])\n",
    "    axes[1].set_title(f'Boxplot of {col} by depressiveness')\n",
    "\n",
    "\n",
    "\n",
    "    # Titles of the axis\n",
    "    axes[1].set_xlabel(col)\n",
    "    axes[1].set_ylabel('')\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat_columns = ['who_bmi', 'anxiety_severity']\n",
    "hue_column = 'depressiveness'\n",
    "\n",
    "# Bar- and Pis-plot for all the categorical features, based on the hue \"depressiveness\"\n",
    "for col in cat_columns:\n",
    "    # Barplot \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=col, hue=hue_column)\n",
    "    plt.title(f'Distrution of {col} with {hue_column}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Number')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    # Pieplo\n",
    "\n",
    "    hue_levels = df[hue_column].unique()\n",
    "    for level in hue_levels:\n",
    "        counts = df[df[hue_column] == level][col].value_counts()\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title(f'Procetual distribution of the {col} for {hue_column} = {level}')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_aim = ['depressiveness']\n",
    "\n",
    "# Bar- and Pie-Plots for the target\n",
    "for col in cat_aim:\n",
    "    # Daten für den Plot vorbereiten\n",
    "    counts = df[col].value_counts()\n",
    "\n",
    "    # Barplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title(f'Distribution of the {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Anzahl')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    # Pieplot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title(f'Precentage Distribution of the columns {col}')\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Columns and their relation, e.g., for the PCA\n",
    "\n",
    "#num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "#            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_cols_no_pca = ['gender', 'bmi']\n",
    "\n",
    "num_cols_pca = ['school_year', 'age', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "# (depression_treatment and anxiety_treatment  are columns, which are used in the target treatment_status)\n",
    "\n",
    "#cat_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "cat_cols = ['who_bmi', 'anxiety_severity']\n",
    "\n",
    "target = ['depressiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important numerical features:\n",
    "#\n",
    "num_cols_import = num_cols_no_pca + num_cols_pca\n",
    "\n",
    "# Pairwise plot for the numerical columns Plot give with the target depressiveness\n",
    "sns.pairplot(df_clean[num_cols_import + target], hue='depressiveness')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Train-Test-Split\n\nIn diesem Projekt brauchst du sowohl die Trainings- als auch die Test- und Zieldaten. \n\nDie Testdaten haben wir nicht vorgegeben. Deshalb empfehlen wir dir an dieser Stelle, die Daten in ein Trainings- und Testset zu teilen und mit dem Trainingsset so zu arbeiten, als wären dies alle Daten, die du zur Verfügung hast. Wenn du dein Modell dann fertig gebaut hast, kannst du mit dem Testset simulieren, was passiert, wenn neue Daten in deine Datenpipeline hereinkommen, also beispielsweise neue Autos auf Auktionsplattformen angeboten werden.\n\nDenke immer daran: **Always fit on Train Set only!** Das gilt inbesondere auch für die Datenbereinigung und das *Feature Engineering*. Im Idealfal fasst du dein Testset nur einmal an, und zwar, wenn du ein für dich optimales Modell gebaut und evaluiert hast und wissen möchtest, wie gut es auf ungesehenen Daten performt. Stell dir einfach vor, du hättest das Testset gar nicht vorliegen.\n\nNutze `train_test_split` aus dem Submodul `sklearn.model_selection`, um die Daten in Test- und Trainingsset aufzuteilen. Übergebe die folgenden Parameter: `random_state=42` und `test_size=0.1`, damit du deine Vorhersagen später mit unserem Modell vergleichen kannst und eine erste Einschätzung erhältst. Speichere zusätzlich `features_test` als *features_test.csv* ab.",
    "selectable": true
   },
   "source": [
    "## Train-Test-Split\n",
    "\n",
    "Splitt of the train and test-dates\n",
    "\n",
    "**Always fit on Train Set only!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "`train_test_split()` haben wir in *Modul 2, Kapitel 4, Vektorisierung von Texten* kennengelernt.\n\nTeile deine Daten zuerst in Feature-Matrix und Zielvektor auf. Für jede Variable, die du überreichst, erhältst du 2 zurück: die Trainingsdaten und die Testdaten. Die Rückgabewerte sollten also aus `features_train`, `features_test`, `target_train`, `target_test` bestehen. ",
    "hint_counter": 2,
    "original-content": "# perform train-test-split\n",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data (start from the dataset)\n",
    "df = pd.read_csv(\"data/depression_anxiety_data.csv\")\n",
    "\n",
    "\n",
    "# perform train-test-split\n",
    "\n",
    "\n",
    "# Feature-Matrix und Zielvektor definieren\n",
    "features = df.drop(columns=['depressiveness'])\n",
    "target = df['depressiveness']\n",
    "\n",
    "# Daten aufteilen in Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                            target, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features_train)\n",
    "display(target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Vergiss beim Speichern nicht `index=False` als Parameter mitzugeben. Ansonsten wird `pandas` eine zusätzliche Spalte mit dem Index erzeugen, wodurch du mehr Spalten als beabsichtigt erhältst.",
    "hint_counter": 1,
    "original-content": "# save features_test as 'features_test.csv'\n",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save features_test as 'features_test.csv'\n",
    "\n",
    "# Testdaten speichern\n",
    "# features_test.to_csv('features_test.csv', index=False)\n",
    "\n",
    "\n",
    "# I first clean the data-set, while we have in the target of the test also NaN, therefore I clean all and save ist after that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Data Preparation\n",
    "selectable": false
   },
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Die *Data Preparation* hat das Ziel, einen Weg zu finden, die Datensätze für dein Modell zu säubern (*Data Cleaning*) und in ein für dein Modell lesbares Format zu bringen (*Datatype Transformation*). Wenn diese Schritte vollzogen sind, kannst du dich daran machen, ein Möglichst repräsentatives Trainingsset auszuwählen (*Sampling*).",
    "selectable": false
   },
   "source": [
    "We apply the *Data Preparation*, e.g., clean the dates and transprm the dates, e.g., special data-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Datatype Transformation",
    "selectable": false
   },
   "source": [
    "### Datatype Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Transformation Function\n",
    "def clean_data(X, y=None):\n",
    "    \"\"\"Cleans and transforms the input feature DataFrame and optionally the target.\n",
    "\n",
    "    This function performs the following operations on the feature DataFrame (X):\n",
    "        - Drops rows with any missing values (NaNs) in either X or y to ensure they remain aligned.\n",
    "        - Converts binary categorical features into numerical values (0 and 1).\n",
    "        - Optionally, cleans and transforms the target DataFrame (y) if provided.\n",
    "        - Converts datetime features to the appropriate datetime format.\n",
    "        - Returns the cleaned features and optionally the cleaned target.\n",
    "\n",
    "    Args: \n",
    "        X (pd.DataFrame): The feature data.\n",
    "        y (pd.DataFrame, optional): The target data. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with transformed features.\n",
    "        pd.DataFrame: Optionally, the cleaned target DataFrame if y is provided.\n",
    "    \"\"\"\n",
    "    # Concatenate X and y to handle missing values across both simultaneously\n",
    "    if y is not None:\n",
    "        # Combine X and y into a single DataFrame for joint NaN removal\n",
    "        combined = pd.concat([X, y], axis=1)\n",
    "        \n",
    "        # Drop rows with any missing values in either X or y\n",
    "        combined_clean = combined.dropna()\n",
    "        \n",
    "        # Separate X and y again\n",
    "        X_clean = combined_clean.iloc[:, :X.shape[1]]\n",
    "        y_clean = combined_clean.iloc[:, X.shape[1]:]\n",
    "    else:\n",
    "        # If y is not provided, only clean X\n",
    "        X_clean = X.dropna()\n",
    "\n",
    "    # List of categorical columns with binary values that need to be converted to integers\n",
    "    cat_cols_trans = ['gender', 'depression_diagnosis', 'depression_treatment',  \n",
    "                      'anxiety_diagnosis', 'sleepiness']\n",
    "\n",
    "    # Map 'gender' column to integers: 'male' -> 1, 'female' -> 0\n",
    "    X_clean['gender'] = X_clean['gender'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    # Convert the specified binary categorical columns in X to integers (0 and 1)\n",
    "    X_clean[cat_cols_trans] = X_clean[cat_cols_trans].astype(int)\n",
    "\n",
    "    if y is not None:\n",
    "        # Convert all target variables to integers\n",
    "        y_clean = y_clean.astype(int)\n",
    "        # Return both cleaned X and cleaned y\n",
    "        return X_clean, y_clean\n",
    "    else:\n",
    "        # If y is not provided, return only the cleaned features\n",
    "        return X_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, target_train = clean_data(features_train, target_train)\n",
    "\n",
    "display(features_train.head())\n",
    "display(target_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the test-data-set\n",
    "features_test, target_test = clean_data(features_test, target_test)\n",
    "\n",
    "display(features_test.head())\n",
    "display(target_test.head())\n",
    "\n",
    "# save features_test as 'features_test.csv'\n",
    "\n",
    "# Testdaten speichern\n",
    "features_test.to_csv('features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Data Imputation",
    "selectable": false
   },
   "source": [
    "### Data Imputation\n",
    "\n",
    "\n",
    "#It is not necessary while we have a very simple data-set, where all the dates are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "print('Columns with NaN')\n",
    "print('is NaN', features_train.isna().sum())\n",
    "\n",
    "# Wir übernehmen den KNN Imputer:\n",
    "\n",
    "\n",
    "\n",
    "# Possible imputer if necessary\n",
    "\"\"\" num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and Transform the training-data\n",
    "features_train_transformed = num_transformer.fit_transform(features_train)\n",
    "\n",
    "# fit and transorm the test-data\n",
    "features_test_transformed = num_transformer.transform(features_test)\n",
    "\n",
    "# Show the results\n",
    "print(\"Transformed training-dates:\\n\", features_train_transformed)\n",
    "print(\"Transformed testdates:\\n\", features_test_transformed) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# Pipeline for the imputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_cols_no_pca = ['gender', 'bmi']\n",
    "\n",
    "num_cols_pca = ['school_year', 'age', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "cat_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#num_pipe = Pipeline(steps=[\n",
    "#    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "#])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Warnung vermeiden durch Kopieren des DataFrames, sher wichtig sonst funktioniert es nicht!\n",
    "features_train = features_train.copy()\n",
    "target_train = target_train.copy()\n",
    "\n",
    "\n",
    "# Spalten-Transformator\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Erstellen einer kompletten Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fitte die Pipeline auf die Trainingsdaten\n",
    "features_train_transformed = pipeline.fit_transform(features_train)\n",
    "\n",
    "# Transformiere die Testdaten\n",
    "features_test_transformed = pipeline.transform(features_test)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"Transformierte Trainingsdaten:\\n\", features_train_transformed)\n",
    "print(\"Transformierte Testdaten:\\n\", features_test_transformed)\n",
    "\n",
    "\n",
    "# Überprüfen auf NaN-Werte\n",
    "print('Anzahl der NaN-Werte in den transformierten Trainingsdaten:', np.isnan(features_train_transformed).sum())\n",
    "print('Anzahl der NaN-Werte in den transformierten Testdaten:', np.isnan(features_test_transformed).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Deal with outliers",
    "selectable": false
   },
   "source": [
    "### Deal with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hint_counter": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of the datasets\n",
    "print(target_train.value_counts())\n",
    "\n",
    "# Optional: precentage distribution of the dataset\n",
    "print(target_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Um deine Bereinigungsschritte leicht mit den Test- und Zieldaten zu reproduzieren, solltest du eine Funktion definieren, welche die jeweiligen Schritte für dich ausführt. Nenne deine Funktion `clean_data`. Sie sollte den unbereinigten `pandas.DataFrame` als Argument erhalten und den gereinigten `pandas.DataFrame` ausgeben. Achte dabei darauf, dass keine Datenpunkte gelöscht werden, denn im Zieldatenset solltest du für jeden Wert eine Vorhersage machen.",
    "selectable": true
   },
   "source": [
    "We apply the DBScan to detect the outliers. We saw, that we have only few outliers, which can be important and do not clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def identify_outliers_dbscan(X_train, correlation_threshold, num_cols, eps=0.5, min_samples=5, outlier_percentage=0.1):\n",
    "    outlier_mask = pd.Series([False] * X_train.shape[0], index=X_train.index)\n",
    "    columns = num_cols\n",
    "    all_outlier_indices = []\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_x = columns[i]\n",
    "            col_y = columns[j]\n",
    "            \n",
    "            # Berechnen des Korrelationskoeffizienten\n",
    "            correlation, _ = pearsonr(X_train[col_x], X_train[col_y])\n",
    "            \n",
    "            # Anwenden von DBSCAN nur bei hoher Korrelation\n",
    "            if abs(correlation) >= correlation_threshold:\n",
    "                X = X_train[[col_x, col_y]].values\n",
    "                \n",
    "                # DBSCAN anwenden\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                dbscan.fit(X)\n",
    "                labels = dbscan.labels_\n",
    "                \n",
    "                # Ausreißer sind die Punkte mit dem Label -1\n",
    "                outliers = labels == -1\n",
    "                all_outlier_indices.extend(X_train.index[outliers])\n",
    "    \n",
    "    # Begrenzen der Anzahl der Ausreißer auf den gewünschten Prozentsatz\n",
    "    all_outlier_indices = np.unique(all_outlier_indices)\n",
    "    num_outliers = int(len(X_train) * outlier_percentage)\n",
    "    \n",
    "    if len(all_outlier_indices) > num_outliers:\n",
    "        np.random.seed(0)  # Für Wiederholbarkeit\n",
    "        selected_outliers = np.random.choice(all_outlier_indices, num_outliers, replace=False)\n",
    "    else:\n",
    "        selected_outliers = all_outlier_indices\n",
    "    \n",
    "    outlier_mask.loc[selected_outliers] = True\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "\n",
    "# important Columns:\n",
    "\n",
    "num_cols_import = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "# Anwenden von identify_outliers_dbscan und Begrenzen auf 10% der Top Outliers\n",
    "outlier_mask = identify_outliers_dbscan(features_train, correlation_threshold=0.7, \n",
    "                                        num_cols=num_cols_import, eps=0.7, min_samples=3, outlier_percentage=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_print = num_cols_import\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot der Daten mit Ausreißern als Hue\n",
    "def plot_data_with_outliers(X_train, outlier_mask, title):\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_train_copy['Outlier'] = outlier_mask\n",
    "    sns.pairplot(X_train_copy, hue='Outlier', vars=num_cols_print, diag_kind='kde', palette={True: \"red\", False: \"blue\"})\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_data_with_outliers(features_train, outlier_mask, \"Daten mit Ausreißern markiert\")\n",
    "\n",
    "# Entfernen der Ausreißer\n",
    "features_train_no_outliers = features_train[~outlier_mask]\n",
    "\n",
    "# Um sicherzustellen, dass keine Spalten mit 0 Varianz vorhanden sind, können Sie diese vor dem Plotten überprüfen\n",
    "def plot_data_no_outliers(X_train, num_cols_print, title):\n",
    "    for col in num_cols:\n",
    "        if X_train[col].nunique() <= 1:\n",
    "            print(f\"Skipping {col} as it has 0 variance\")\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=X_train, x=col, hue='depressiveness', kde=True, multiple=\"stack\")\n",
    "        plt.title(f'Histogram of {col} by depressiveness')\n",
    "        plt.show()\n",
    "\n",
    "plot_data_with_outliers(features_train_no_outliers, outlier_mask[~outlier_mask], \"Daten nach Entfernen der Ausreißer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of outliers\n",
    "num_outliers = outlier_mask.sum()\n",
    "print(f\"Number of Outliers: {num_outliers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Resample",
    "selectable": false
   },
   "source": [
    "### Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Wie du sicherlich festgestellt hast, sind die Zielkategorien im Datensatz sehr unausgeglichen. Es kann also notwendig sein, dein Trainingsdatenset zu resamplen. In *Unausgeglichene Zeilkategorien* (Modul 2, Kapitel 3) hast du verschiedene Methoden des *resampling* kennengelernt und auch gelernt, wie man `imblearn.pipeline` nutzt, um verschiedene Samplingmethoden zu testen.",
    "selectable": true
   },
   "source": [
    "We apply an oversampler to balance the target colum.\n",
    "\n",
    "*resampling* uses the `imblearn.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the resampling in the pipeline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# We apply an oversampler\n",
    "#rf_pipe = ImbPipeline(steps=[\n",
    "#    (\"pre\", preprocessing),\n",
    "#    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers\n",
    "#    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "#    (\"model\", RandomForestClassifier()),\n",
    "#])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the unbalance of the target and apply it to the pipeline\n",
    "print(target_train.value_counts())\n",
    "\n",
    "# Percantage of the imbalance\n",
    "print(target_train.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "display(features_train)\n",
    "\n",
    "display(features_train.dtypes)\n",
    "\n",
    "\n",
    "display(target_train)\n",
    "\n",
    "display(target_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Modeling",
    "selectable": false
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Kommen wir nun zur Kernaufgabe eines *Data Scientist*, dem Bauen von Maschine-Learning-Modellen. Wie du es im Training bereits gelernt hast, ist das Optimieren eines Modells ein iterativer Prozess. Die nachfolgenden Schritte wirst du also sicher mehr als einmal durchführen.",
    "selectable": false
   },
   "source": [
    "Kommen wir nun zur Kernaufgabe eines *Data Scientist*, dem Bauen von Maschine-Learning-Modellen. Wie du es im Training bereits gelernt hast, ist das Optimieren eines Modells ein iterativer Prozess. Die nachfolgenden Schritte wirst du also sicher mehr als einmal durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Build a simple Baselinemodel",
    "selectable": false
   },
   "source": [
    "### Build a simple Baselinemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Bevor du dich nun der Erzeugung und Auswahl neuer Features widmest, solltest du erst einmal sehen, von welchem Punkt du deine Bemühungen startest und ein erstes einfaches Modell ohne jegliche Hyperparameteroptimierung erzeugen. So kannst du hinterher immer wieder überprüfen, ob die folgenden Schritte in der Modelloptimierung eine Verbesserung bringen. Am besten erzeugst du eine Pipeline, die neben der Modellierung auch das Encoden nicht-numerischer Features übernimmt. Ich schlage folgendes Vorgehen vor:\n1. Numerische Features (`num_cols`) und kategorische Features (`cat_cols`) definieren,\n3. Algorithmus für das Baselinemodel aussuchen und als `model` instanziieren,\n3. Pipeline (`model_baseline`) nach dem Schema unten erstellen,\n4. Das gereinigte und ggf. gefilterte `features_train` sowie das entsprechende `target _train` zum *fitten* verwenden,\n5. mit `model_baseline` Vorhersagen treffen auf das gereinigte `features_test`.\n\n<img src=\"03_05_01_pic1_baselinepipe.png\">",
    "selectable": true
   },
   "source": [
    "We built a baseline model:\n",
    "\n",
    "1. Numerical features (`num_cols`) and categorical features (`cat_cols`),\n",
    "3. We apply a baseline model,\n",
    "3. We apply a pipeline (`model_baseline`),\n",
    "4. We apply the `features_train` and the `target _train` for the  *fitting* ,\n",
    "5. We apply the `model_baseline` to predict the `features_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# define num_cols and cat_cols\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# define num_cols and cat_cols\n",
    "\n",
    "\n",
    "\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_cols_no_pca = ['gender', 'bmi']\n",
    "\n",
    "num_cols_pca = ['school_year', 'age', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "#targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "cat_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = ['depressiveness']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definieren der Spaltenlisten für die depressiveness\n",
    "\n",
    "\n",
    "#num_2_cols = ['school_year', 'age', 'gender', 'bmi', 'phd_score', 'depression_diagnosis', 'gad_score',\n",
    "#            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_2_cols = ['school_year', 'age', 'gender', 'bmi', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# categorical columns\n",
    "cat_2_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "cat_2_cols = ['who_bmi', 'anxiety_severity']\n",
    "\n",
    "\n",
    "# number of categories for each column\n",
    "for col in cat_2_cols:\n",
    "    num_categories = df[col].nunique()\n",
    "    print(f'Number of cattegories in {col}: {num_categories}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# instantiate model\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des TargetEncoder von category_encoder\n",
    "\n",
    "!pip install category_encoders\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_2_cols:\n",
    "    features_train[col] = features_train[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cat_2_cols = ['anxiety_severity','who_bmi']\n",
    "\n",
    "# Ensure target_train is a numpy array or a pandas Series\n",
    "target_train = np.array(target_train)\n",
    "\n",
    "\n",
    "numeric_features = num_2_cols\n",
    "categorical_features = cat_2_cols\n",
    "\n",
    "# Preprocessing Pipelines für numerische und kategorische Daten\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', ce.TargetEncoder())])\n",
    "\n",
    "# ColumnTransformer zum Kombinieren der Preprocessing-Schritte\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "# Erstellen der Pipeline mit Preprocessing und dem Modell\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Fit auf die Trainingsdaten\n",
    "rf_pipe.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# build pipeline\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Eigene Klasse mit Zeiterfassung\n",
    "\n",
    "\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"target_enc\", ce.TargetEncoder())  # TargetEncoder hinzufügen\n",
    "#    (\"ohe\", OneHotEncoder(handle_unknown='ignore')),  # One-Hot encoder\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "#    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # PolynomialFeatures hinzufügen, bringt nichts, da man\n",
    "#    Lineare Kombinationen hat\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_2_cols),\n",
    "        (\"cat_trans\", cat_pipe, cat_2_cols),\n",
    "    ],\n",
    "    remainder = \"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "# Resampling\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# In meine Pipeline habe ich den RandomOverSapler eingebout \n",
    "\n",
    "\n",
    "rf_pipe = ImbPipeline(steps=[\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers\n",
    "    (\"pre\", preprocessing),\n",
    "    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "    (\"model\", RandomForestClassifier()),\n",
    "#    (\"model\", RandomForestClassifier_mod()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# fit pipeline on cleaned (and filtered) training set\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# fit pipeline on cleaned (and filtered) training set\n",
    "\n",
    "rf_pipe.fit(features_train, target_train)  # Standard\n",
    "\n",
    "# rf_pipe_mod.fit(features_train, target_train)  # Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# predict and evaluate on test set\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# predict and evaluate on test set\n",
    "\n",
    "print(classification_report(target_test, rf_pipe.predict(features_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Manually close the current log handlers\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Ensure the 'log' directory exists\n",
    "log_dir = 'log'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Confirm that the directory was created\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"Directory '{log_dir}' exists or was successfully created.\")\n",
    "else:\n",
    "    print(f\"Failed to create directory '{log_dir}'.\")\n",
    "\n",
    "# Check the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Configure logging explicitly\n",
    "log_file_path = os.path.join(log_dir, 'model_evaluation.log')\n",
    "print(f\"Log file path: {log_file_path}\")  # This should point to the 'log' directory\n",
    "\n",
    "# Create and configure the logger explicitly\n",
    "logger = logging.getLogger('model_evaluation_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Avoid duplicate logs by clearing existing handlers if any\n",
    "\n",
    "if logger.hasHandlers():\n",
    "\n",
    "    logger.handlers.clear()\n",
    "\n",
    "\n",
    "# Create a file handler for logging\n",
    "\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Create a logging format\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "# Add the file handler to the logger\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "# Example log entry\n",
    "\n",
    "logger.info(\"Logging to the log/model_evaluation.log file.\")\n",
    "\n",
    "print(\"Logging setup completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Model name\n",
    "model_name = type(rf_pipe.named_steps['model']).__name__\n",
    "\n",
    "# Model parameters\n",
    "model_params = rf_pipe.named_steps['model'].get_params()\n",
    "\n",
    "# Classification report\n",
    "classification_rep = classification_report(target_test, rf_pipe.predict(features_test))\n",
    "\n",
    "# Log the model details\n",
    "logger.info(\"Model Evaluation\")\n",
    "logger.info(f\"Model Name: {model_name}\")\n",
    "logger.info(\"Model Parameters:\")\n",
    "for param, value in model_params.items():\n",
    "    logging.info(f\"{param}: {value}\")\n",
    "\n",
    "# Log the classification report\n",
    "logger.info(\"Classification Report:\")\n",
    "logger.info(\"\\n\" + classification_rep)\n",
    "\n",
    "print(f\"Model details and evaluation have been logged to 'log\\model_evaluation.log'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Assuming 'model' is your trained model and 'X' and 'y' are your features and target\n",
    "scores = cross_val_score(rf_pipe, features_train, target_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Cross-Validation Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "print(rf_pipe.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Importance ermitteln\n",
    "rf_model = rf_pipe.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen nach der Transformation\n",
    "preprocessor = rf_pipe.named_steps['pre']\n",
    "kbest = rf_pipe.named_steps['kbest']\n",
    "selected_features = kbest.get_support(indices=True)\n",
    "\n",
    "# Numerische und kategorische Feature-Namen nach der Transformation\n",
    "num_feature_names = num_2_cols\n",
    "cat_feature_names = cat_2_cols\n",
    "\n",
    "# Kombinierte Liste der ursprünglichen Features\n",
    "all_features = num_feature_names + cat_feature_names\n",
    "\n",
    "# Ausgewählte Features nach SelectKBest\n",
    "selected_feature_names = [all_features[i] for i in selected_features]\n",
    "\n",
    "# Erzeuge eine Serie mit den Feature Importances\n",
    "feature_importance_rf = pd.Series(rf_model.feature_importances_, index=selected_feature_names)\n",
    "\n",
    "# Sortiere die Werte und wähle die Top 10 Features aus\n",
    "top_10_features = feature_importance_rf.nlargest(10)\n",
    "\n",
    "# Erzeuge ein Balkendiagramm\n",
    "plt.style.use('fivethirtyeight')\n",
    "top_10_features.plot(kind='barh', figsize=(10, 8))\n",
    "\n",
    "# Füge Beschriftungen hinzu\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in Random Forest')\n",
    "\n",
    "# Zeige das Diagramm an\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sortiere die Werte und wähle die Top 10 Features aus\n",
    "top_10_features = feature_importance_rf.nlargest(10).sort_values()\n",
    "\n",
    "# Erzeuge ein Balkendiagramm für die Top 10 Features\n",
    "plt.style.use('fivethirtyeight')\n",
    "top_10_features.plot(kind='barh', figsize=(10, 8))\n",
    "\n",
    "# Füge Beschriftungen hinzu\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 10 Feature Importance in Random Forest')\n",
    "\n",
    "# Zeige das Diagramm an\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fist Idea of a grid search:\n",
    "\n",
    "# Modified GridSearch\n",
    "\n",
    "#class GridSearchCV_mod(GridSearchCV):\n",
    "#    @timeit\n",
    "#    def fit(self, X, y=None, **fit_params):\n",
    "#        return super().fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameter Grid definieren für RF_mod\n",
    "#param_grid = {\n",
    "#    'model__n_estimators': [100],  # , 200],  # , 300],\n",
    "#    'model__max_features': [None, 'sqrt'],  # , 'log2'],\n",
    "#   'model__max_depth': [10, 20],  # , 30, None],\n",
    "#    'model__min_samples_split': [2, 5],  # , 10],\n",
    "#    'model__min_samples_leaf': [2, 4],  # [1, 2, 4]\n",
    "#    'model__max_samples_leaf': [0.1, 0.2],  # Beispielsweise zwei Werte zur Auswahl\n",
    "#}\n",
    "\n",
    "# Parameterraum mit normalem RF\n",
    "#param_grid = {\n",
    "#    'model__n_estimators': [100] , # , 200],  # , 300],\n",
    "#    'model__max_features': [None, 'sqrt'],  # , 'log2'],\n",
    "#    'model__max_depth': [10, 20],  # , 30, None],\n",
    "#    'model__min_samples_split': [2, 5],  # , 10],\n",
    "#    'model__min_samples_leaf': [2, 4],  # [1, 2, 4]\n",
    "#}\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # , 300],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__max_depth': [10],  # , 30, None],\n",
    "    'model__min_samples_split': [2, 5],  # , 10],\n",
    "    'model__min_samples_leaf': [2, 4],  # [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV durchführen # opimise the model after precision\n",
    "\n",
    "\n",
    "#grid_search_rf = GridSearchCV_mod(rf_pipe, param_grid, cv=5, scoring='f1', n_jobs=-1)  # Standard\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_pipe, param_grid, cv=5, scoring='f1', n_jobs=-1)  # Standard\n",
    "\n",
    "#grid_search_rf = GridSearchCV(rf_pipe_mod, param_grid, cv=5, scoring='f1', n_jobs=-1)  # modified\n",
    "\n",
    "grid_search_rf.fit(features_train, target_train)\n",
    "\n",
    "# Beste Parameter und Ergebnis anzeigen\n",
    "print(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "print(f'Best score: {grid_search_rf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate on test set\n",
    "\n",
    "print(classification_report(target_test, grid_search_rf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log the best parameters and the best score\n",
    "logger.info('GridSearchCV Results')\n",
    "logger.info(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "logger.info(f'Best score: {grid_search_rf.best_score_}')\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred = grid_search_rf.predict(features_test)\n",
    "classification_rep = classification_report(target_test, y_pred)\n",
    "\n",
    "# Log the classification report\n",
    "logger.info('Classification Report on Test Set:')\n",
    "logger.info(\"\\n\" + classification_rep)\n",
    "\n",
    "# Print to console for quick reference\n",
    "print(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "print(f'Best score: {grid_search_rf.best_score_}')\n",
    "print(classification_report(target_test, y_pred))\n",
    "\n",
    "print(\"Results have been logged to 'log\\model_evaluation.log'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with an aim-Dataset\n",
    "\n",
    "\n",
    "#  Example aim-file\n",
    "\n",
    "\n",
    "# Reading the CSV file into a DataFrame\n",
    "X_aim = pd.read_csv('aim_test.csv')\n",
    "\n",
    "\n",
    "# Cleaning data-file\n",
    "X_aim = clean_data(X_aim)\n",
    "\n",
    "\n",
    "display(X_aim)\n",
    "y_pred_aim = grid_search_rf.predict(X_aim)\n",
    "print(X_aim)\n",
    "print(y_pred_aim)\n",
    "\n",
    "# 0 is not depressive\n",
    "# 1 is depressiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'depressiveness'\n",
    "\n",
    "print(target_col)\n",
    "\n",
    "\n",
    "# Sort the values and select the top 10 features\n",
    "top_10_features_series = feature_importance_rf.nlargest(10).sort_values()\n",
    "\n",
    "# Convert the top 10 features into a list of tuples\n",
    "top_10_features = list(top_10_features_series.items())\n",
    "\n",
    "# Extract just the feature names into a list\n",
    "top_10_feature_names = top_10_features_series.index.tolist()\n",
    "\n",
    "# Print the list of top 10 feature names for verification\n",
    "print(\"Top 10 Feature Names:\", top_10_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#  Save with pickle\n",
    "\n",
    "\n",
    "with open('model_depression.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search_rf, file)\n",
    "    pickle.dump(target_col, file)\n",
    "#    pickle.dump(top_10_feature_names, file)\n",
    "    pickle.dump(all_features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of one Data-Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example input data\n",
    "aim = [19, 33.33, 'Class I Obesity', 0, 0, 1, 1, 0, 7, 'Moderate', 11, 'Mild', 9]\n",
    "\n",
    "# Define column names\n",
    "columns = ['age', 'bmi', 'who_bmi', 'sleepiness', 'anxiety_diagnosis', 'gender', 'school_year', \n",
    "           'depression_diagnosis', 'epworth_score', 'anxiety_severity', \n",
    "           'gad_score', 'depression_severity', 'phq_score']\n",
    "\n",
    "# Convert aim into a DataFrame\n",
    "X_aim = pd.DataFrame([aim], columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "display(X_aim)\n",
    "y_pred_aim = grid_search_rf.predict(X_aim)\n",
    "print(X_aim)\n",
    "print(y_pred_aim)\n",
    "\n",
    "# 0 is not depressive\n",
    "# 1 is depressiv\n"
   ]
  }
 ],
 "metadata": {
  "content_id": "5546f3fd-971f-4387-824c-375dfcedfa49",
  "content_language": "de",
  "content_title": "Abschlussprojekt: Gebrauchtwagenkäufe",
  "content_type": "exercise",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
