{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "# Abschlussprojekt: Gebrauchtwagenkäufe (Lösung)\nModul 3 | Kapitel 4 | Notebook 1\n\nDu hast in den letzten Kapiteln und Modulen zahlreiche Data-Science-Methoden kennengelernt. Nachdem du im Übungsprojekt eine Regression auf Daten mit relativ wenigen Features durchgeführt hast, kannst du dich nun im Abschlussprojekt der Klassifizierung widmen. Dazu erhältst du ein Datenset, welches Gebrauchtwagenkäufe und unausgeglichene Zielkategorien beinhaltet.\n\n***\nAm Ende der Übung hast du:\n* Daten eingelesen und bereinigt,\n* Feature Engineering betrieben,\n* ein Modell an die Daten angepasst,\n* die einzelnen Schritte zu einer Datenpipeline kombiniert,\n* dein Modell interpretiert.\n***",
    "selectable": false
   },
   "source": [
    "# Portfolioprojekt: Depression prediction with the best mdel (method-file to choose the best model)\n",
    "\n",
    "The project was given from:\n",
    "\n",
    "https://www.kaggle.com/datasets/shahzadahmad0402/depression-and-anxiety-data/code\n",
    "\n",
    "\n",
    "***\n",
    "We have the following tasks:\n",
    "* Data reading and cleaning,\n",
    "* Feature Engineering,\n",
    "* Optimal model, fit the dates to the model and grid-search of different models,\n",
    "* Combination to the pipeline,\n",
    "* Chossing the best model with the optimal columns.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-set is given with th columns\n",
    "\n",
    "| **Column** | **Description** |\n",
    "| ------------ | :-----------------: |\n",
    "| id | each number is a participant in the experiment |\n",
    "| school_year | years in school |\n",
    "| age | |\n",
    "| gender | |\n",
    "| bmi | body mass index |\n",
    "| who_bmi | bmi category |\n",
    "| phq_score | measure the severity of symptoms related to depression, anxiety, and other related disorders in patients |\n",
    "| depression_severity | degree or intensity of symptoms experienced by an individual with depression |\n",
    "| depressiveness | |\n",
    "| suicidal | the candidate have suicide thought |\n",
    "| depression_diagnosis | the candidate already have depression diagnosis |\n",
    "| depression_treatment | the candidate already have depression treatment |\n",
    "| gad_score | measure that assesses the severity of Generalized Anxiety Disorder |\n",
    "| anxiety_severity |  intensity of symptoms experienced by an individual with anxiety |\n",
    "| anxiousness | |\n",
    "| anxiety_diagnosis | the candidate already have anxiety diagnosis |\n",
    "| anxiety_treatment | the candidate already have anxiety treatment |\n",
    "| epworth_score |  score to assess daytime sleepiness ytime sleepiness |\n",
    "| sleepiness | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Define Metric",
    "selectable": false
   },
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Anhand deines Verständnisses des vorliegenden Problems solltet du dir nun überlegen, welche Metrik(en) am besten geeignet sind, den Erfolg deines Modells zu beurteilen.",
    "selectable": false
   },
   "source": [
    "We apply the F1-matric to get a balance between the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "\n",
    "# Metrics considerations\n",
    "\n",
    "# Accuracy: The proportion of correct predictions. \n",
    "            # In your case this metric might be less important, \n",
    "           # I may have an unbalanced dataset (more good purchases than bad purchases).\n",
    "# Better metrics\n",
    "# Recall (Sensitivity): The proportion of correctly identified bad purchases out of all \n",
    "# actual bad purchases. Here I optimize: Identify as many bad purchases as possible.\n",
    "# Precision: The proportion of correctly identified bad purchases out of all as bad purchases \n",
    "# predicted cars. Important to minimize false purchases.\n",
    "# It is best to weight between the two:\n",
    "# F1-Score: The harmonic mean of precision and recall. \n",
    "# Here I have to find a balance between precision and recall.\n",
    "\n",
    "# We apply the F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Gather Data",
    "selectable": false
   },
   "source": [
    "### Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Die Daten befinden sich in der Datei *data_train.csv*. Der Zielvektor ist durch die Spalte `'IsBadBuy'` gegeben. Importiere die Module, die du typischerweise für das Einlesen und die Exploration benötigst und lies die Daten anschließend ein. ",
    "selectable": true
   },
   "source": [
    "We have the dataset: *depression_anxiety_data.csv*. With the target-column: 'depressiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Importiere z.B. die Module `pandas`, `numpy`, `seaborn` und `matplotlib.pyplot`.",
    "hint_counter": 0,
    "original-content": "# import modules \n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# import modules \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Columntransormer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# OneHotEndode,, Scaler\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Train/Test Splitt, Crossvaue-Score, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# read data\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# Target encoder\n",
    "\n",
    "# !pip install category_encoders  # done in yml\n",
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## EDA",
    "selectable": false
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "messageType": "glueckwunsch",
    "original-content": "**Glückwunsch**:\nDu hast nun einen besseren Einblick in deine Daten! Dieser kann dir beim weiteren Vorgehen weiterhelfen.",
    "selectable": false
   },
   "source": [
    "**EDA**:\n",
    "Here, we apply only a brief EDA, while the full EDA is done on the EDA-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read data (start from the dataset)\n",
    "df = pd.read_csv(\"../data/depression_anxiety_data.csv\")\n",
    "\n",
    "display(df.head())\n",
    "#print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Here, we apply the cleaing of the data based on our EDA in the \"eda_model_depression.ipynb\"  file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of the full data set, before the splitt based on the targets\n",
    "# Optional \n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# def clean_data_target(df, target_columns=None):\n",
    "#     \"\"\"\n",
    "#     Cleans and transforms the input DataFrame.\n",
    "\n",
    "#     This function performs the following operations on the DataFrame:\n",
    "#         - Drops rows with any missing values (NaNs) in the specified target columns.\n",
    "#         - Converts binary categorical features into numerical values (0 and 1).\n",
    "#         - Converts datetime features to the appropriate datetime format.\n",
    "#         - Returns the cleaned DataFrame.\n",
    "\n",
    "#     Args: \n",
    "#         df (pd.DataFrame): The data to clean.\n",
    "#         target_columns (list of str, optional): List of target column names for NaN removal. \n",
    "#                                                 If None, no target-specific NaN removal is performed.\n",
    "        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: A cleaned DataFrame with transformed features.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Drop rows with missing values in the specified target columns\n",
    "#     if target_columns:\n",
    "#         df_clean = df.dropna(subset=target_columns)\n",
    "#     else:\n",
    "#         df_clean = df.copy()\n",
    "\n",
    "#     # Further cleaning or transformation can be added here if needed\n",
    "    \n",
    "#     return df_clean\n",
    "\n",
    "# # Define the target of the dataset\n",
    "\n",
    "# target = ['depressiveness']\n",
    "\n",
    "# df = clean_data_target(df, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "\n",
    "\n",
    "# target\n",
    "\n",
    "# depressiveness\n",
    "\n",
    "# binary categories:\n",
    "\n",
    "# gender\tsuicidal\tdepression_diagnosis\tdepression_treatment\tanxiousness\tanxiety_diagnosis\tanxiety_treatment\tsleepiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where epworth_score >= 31\n",
    "filtered_df = df[df['epworth_score'] >= 31]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General data-cleaning after the splitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data Cleaning and Transformation Function, this is done for the train and test datasets\n",
    "def clean_data(X, y=None):\n",
    "    \"\"\"Cleans and transforms the input feature DataFrame and optionally the target.\n",
    "\n",
    "    This function performs the following operations on the feature DataFrame (X):\n",
    "        - Drops rows with any missing values (NaNs) in either X or y to ensure they remain aligned.\n",
    "        - Converts binary categorical features into numerical values (0 and 1).\n",
    "        - Optionally, cleans and transforms the target DataFrame (y) if provided.\n",
    "        - Converts datetime features to the appropriate datetime format.\n",
    "        - If 'epworth_score' >= 31, sets the value to 17.\n",
    "        - Returns the cleaned features and optionally the cleaned target.\n",
    "\n",
    "    Args: \n",
    "        X (pd.DataFrame): The feature data.\n",
    "        y (pd.DataFrame, optional): The target data. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with transformed features.\n",
    "        pd.DataFrame: Optionally, the cleaned target DataFrame if y is provided.\n",
    "    \"\"\"\n",
    "    # Concatenate X and y to handle missing values across both simultaneously\n",
    "    if y is not None:\n",
    "        # Combine X and y into a single DataFrame for joint NaN removal\n",
    "        combined = pd.concat([X, y], axis=1)\n",
    "        \n",
    "        # Drop rows with any missing values in either X or y\n",
    "        combined_clean = combined.dropna()\n",
    "        \n",
    "        # Separate X and y again\n",
    "        X_clean = combined_clean.iloc[:, :X.shape[1]]\n",
    "        y_clean = combined_clean.iloc[:, X.shape[1]:]\n",
    "    else:\n",
    "        # If y is not provided, only clean X\n",
    "        X_clean = X.dropna()\n",
    "\n",
    "    # List of categorical columns with binary values that need to be converted to integers\n",
    "    cat_cols_trans = ['gender', 'suicidal' , 'depression_diagnosis', 'depression_treatment',  \n",
    "                      'anxiousness', 'anxiety_diagnosis', 'anxiety_treatment', 'sleepiness']\n",
    "\n",
    "    # Map 'gender' column to integers: 'male' -> 1, 'female' -> 0\n",
    "    X_clean['gender'] = X_clean['gender'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    # Convert the specified binary categorical columns in X to integers (0 and 1)\n",
    "    X_clean[cat_cols_trans] = X_clean[cat_cols_trans].astype(int)\n",
    "\n",
    "    # If 'epworth_score' >= 31, set the value to 17\n",
    "    X_clean.loc[X_clean['epworth_score'] >= 31, 'epworth_score'] = 17\n",
    "\n",
    "    if y is not None:\n",
    "        # Convert all target variables to integers\n",
    "        y_clean = y_clean.astype(int)\n",
    "        # Return both cleaned X and cleaned y\n",
    "        return X_clean, y_clean\n",
    "    else:\n",
    "        # If y is not provided, return only the cleaned features\n",
    "        return X_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Cleaning and Transformation Function, this is done for the train and test datasets\n",
    "# def clean_data(X, y=None):\n",
    "#     \"\"\"Cleans and transforms the input feature DataFrame and optionally the target.\n",
    "\n",
    "#     This function performs the following operations on the feature DataFrame (X):\n",
    "#         - Drops rows with any missing values (NaNs) in either X or y to ensure they remain aligned.\n",
    "#         - Converts binary categorical features into numerical values (0 and 1).\n",
    "#         - Optionally, cleans and transforms the target DataFrame (y) if provided.\n",
    "#         - Converts datetime features to the appropriate datetime format.\n",
    "#         - Returns the cleaned features and optionally the cleaned target.\n",
    "\n",
    "#     Args: \n",
    "#         X (pd.DataFrame): The feature data.\n",
    "#         y (pd.DataFrame, optional): The target data. Default is None.\n",
    "        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: A cleaned DataFrame with transformed features.\n",
    "#         pd.DataFrame: Optionally, the cleaned target DataFrame if y is provided.\n",
    "#     \"\"\"\n",
    "#     # Concatenate X and y to handle missing values across both simultaneously\n",
    "#     if y is not None:\n",
    "#         # Combine X and y into a single DataFrame for joint NaN removal\n",
    "#         combined = pd.concat([X, y], axis=1)\n",
    "        \n",
    "#         # Drop rows with any missing values in either X or y\n",
    "#         combined_clean = combined.dropna()\n",
    "        \n",
    "#         # Separate X and y again\n",
    "#         X_clean = combined_clean.iloc[:, :X.shape[1]]\n",
    "#         y_clean = combined_clean.iloc[:, X.shape[1]:]\n",
    "#     else:\n",
    "    #     # If y is not provided, only clean X\n",
    "    #     X_clean = X.dropna()\n",
    "\n",
    "    # # List of categorical columns with binary values that need to be converted to integers\n",
    "    # cat_cols_trans = ['gender', 'suicidal' , 'depression_diagnosis', 'depression_treatment',  \n",
    "    #                   'anxiousness', 'anxiety_diagnosis', 'anxiety_treatment', 'sleepiness']\n",
    "\n",
    "    # # Map 'gender' column to integers: 'male' -> 1, 'female' -> 0\n",
    "    # X_clean['gender'] = X_clean['gender'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    # # Convert the specified binary categorical columns in X to integers (0 and 1)\n",
    "    # X_clean[cat_cols_trans] = X_clean[cat_cols_trans].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    # if y is not None:\n",
    "    #     # Convert all target variables to integers\n",
    "    #     y_clean = y_clean.astype(int)\n",
    "    #     # Return both cleaned X and cleaned y\n",
    "    #     return X_clean, y_clean\n",
    "    # else:\n",
    "    #     # If y is not provided, return only the cleaned features\n",
    "    #     return X_clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We apply the data-cleaning after the train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Train-Test-Split\n\nIn diesem Projekt brauchst du sowohl die Trainings- als auch die Test- und Zieldaten. \n\nDie Testdaten haben wir nicht vorgegeben. Deshalb empfehlen wir dir an dieser Stelle, die Daten in ein Trainings- und Testset zu teilen und mit dem Trainingsset so zu arbeiten, als wären dies alle Daten, die du zur Verfügung hast. Wenn du dein Modell dann fertig gebaut hast, kannst du mit dem Testset simulieren, was passiert, wenn neue Daten in deine Datenpipeline hereinkommen, also beispielsweise neue Autos auf Auktionsplattformen angeboten werden.\n\nDenke immer daran: **Always fit on Train Set only!** Das gilt inbesondere auch für die Datenbereinigung und das *Feature Engineering*. Im Idealfal fasst du dein Testset nur einmal an, und zwar, wenn du ein für dich optimales Modell gebaut und evaluiert hast und wissen möchtest, wie gut es auf ungesehenen Daten performt. Stell dir einfach vor, du hättest das Testset gar nicht vorliegen.\n\nNutze `train_test_split` aus dem Submodul `sklearn.model_selection`, um die Daten in Test- und Trainingsset aufzuteilen. Übergebe die folgenden Parameter: `random_state=42` und `test_size=0.1`, damit du deine Vorhersagen später mit unserem Modell vergleichen kannst und eine erste Einschätzung erhältst. Speichere zusätzlich `features_test` als *features_test.csv* ab.",
    "selectable": true
   },
   "source": [
    "## Train-Test-Split\n",
    "\n",
    "In diesem Projekt brauchst du sowohl die Trainings- als auch die Test- und Zieldaten. \n",
    "\n",
    "Die Testdaten haben wir nicht vorgegeben. Deshalb empfehlen wir dir an dieser Stelle, die Daten in ein Trainings- und Testset zu teilen und mit dem Trainingsset so zu arbeiten, als wären dies alle Daten, die du zur Verfügung hast. Wenn du dein Modell dann fertig gebaut hast, kannst du mit dem Testset simulieren, was passiert, wenn neue Daten in deine Datenpipeline hereinkommen, also beispielsweise neue Autos auf Auktionsplattformen angeboten werden.\n",
    "\n",
    "Denke immer daran: **Always fit on Train Set only!** Das gilt inbesondere auch für die Datenbereinigung und das *Feature Engineering*. Im Idealfal fasst du dein Testset nur einmal an, und zwar, wenn du ein für dich optimales Modell gebaut und evaluiert hast und wissen möchtest, wie gut es auf ungesehenen Daten performt. Stell dir einfach vor, du hättest das Testset gar nicht vorliegen.\n",
    "\n",
    "Nutze `train_test_split` aus dem Submodul `sklearn.model_selection`, um die Daten in Test- und Trainingsset aufzuteilen. Übergebe die folgenden Parameter: `random_state=42` und `test_size=0.1`, damit du deine Vorhersagen später mit unserem Modell vergleichen kannst und eine erste Einschätzung erhältst. Speichere zusätzlich `features_test` als *features_test.csv* ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "`train_test_split()` haben wir in *Modul 2, Kapitel 4, Vektorisierung von Texten* kennengelernt.\n\nTeile deine Daten zuerst in Feature-Matrix und Zielvektor auf. Für jede Variable, die du überreichst, erhältst du 2 zurück: die Trainingsdaten und die Testdaten. Die Rückgabewerte sollten also aus `features_train`, `features_test`, `target_train`, `target_test` bestehen. ",
    "hint_counter": 1,
    "original-content": "# perform train-test-split\n",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data (start from the dataset)\n",
    "df = pd.read_csv(\"../data/depression_anxiety_data.csv\")\n",
    "\n",
    "\n",
    "# perform train-test-split\n",
    "\n",
    "\n",
    "# Feature-Matrix und Zielvektor definieren\n",
    "features = df.drop(columns=['depressiveness'])\n",
    "target = df['depressiveness']\n",
    "\n",
    "# Daten aufteilen in Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                            target, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testdaten speichern und Target speichern\n",
    "features_test.to_csv('../data/features_test.csv', index=False)\n",
    "# target_test.to_csv('target_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data:\n",
    "\n",
    "features_train, target_train = clean_data(features_train, target_train)\n",
    "\n",
    "display(features_train.head())\n",
    "display(target_train.head())\n",
    "\n",
    "\n",
    "# Preparation of the test-data-set\n",
    "features_test, target_test = clean_data(features_test, target_test)\n",
    "\n",
    "display(features_test.head())\n",
    "display(target_test.head())\n",
    "\n",
    "# save features_test as 'features_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling:\n",
    "# features_train samplen mit Over\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# manuell den Oversampler ausgefüht\n",
    "\n",
    "\n",
    "# Anwenden des RandomOverSamplers\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "features_resampled, target_resampled = ros.fit_resample(features_train, target_train)\n",
    "\n",
    "print(target_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the 2D array to 1D\n",
    "target_resampled_flattened = target_resampled.values.ravel()\n",
    "\n",
    "# Convert to Pandas Series\n",
    "target_resampled_series = pd.Series(target_resampled_flattened)\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=target_resampled_series)\n",
    "plt.title('Distribution of Target Variable After Resampling')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Data Preparation\n",
    "selectable": false
   },
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Datatype Transformation",
    "selectable": false
   },
   "source": [
    "### Datatype Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "",
    "selectable": true
   },
   "source": [
    "It is done previous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Data Imputation",
    "selectable": false
   },
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the mean for the numerical values and KNN for the catergorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline for the imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "num_cols_no_pca = ['gender', 'bmi']\n",
    "\n",
    "num_cols_pca = ['school_year', 'age', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "cat_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cat_pipe = Pipeline(steps=[\n",
    "#    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "#    (\"ohe\", OneHotEncoder(handle_unknown='ignore')),\n",
    "#])\n",
    "\n",
    "\n",
    "# Pipeline for categorical columns\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),  # Simple Imputer for categorical data\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown='ignore'))  # OneHotEncoding\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#num_pipe = Pipeline(steps=[\n",
    "#    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "#])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#num_pipe = Pipeline(steps=[\n",
    "#    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "#    ('std_scaler', StandardScaler())\n",
    "#])\n",
    "\n",
    "\n",
    "\n",
    "# Warnung vermeiden durch Kopieren des DataFrames, sher wichtig sonst funktioniert es nicht!\n",
    "features_train = features_train.copy()\n",
    "target_train = target_train.copy()\n",
    "\n",
    "\n",
    "# Spalten-Transformator\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Erstellen einer kompletten Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fitte die Pipeline auf die Trainingsdaten\n",
    "features_train_transformed = pipeline.fit_transform(features_train)\n",
    "\n",
    "# Transformiere die Testdaten\n",
    "features_test_transformed = pipeline.transform(features_test)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"Transformierte Trainingsdaten:\\n\", features_train_transformed)\n",
    "print(\"Transformierte Testdaten:\\n\", features_test_transformed)\n",
    "\n",
    "\n",
    "# Überprüfen auf NaN-Werte\n",
    "print('Anzahl der NaN-Werte in den transformierten Trainingsdaten:', np.isnan(features_train_transformed).sum())\n",
    "print('Anzahl der NaN-Werte in den transformierten Testdaten:', np.isnan(features_test_transformed).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Deal with outliers",
    "selectable": false
   },
   "source": [
    "### Deal with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "* Gibt es Ausreißer im Trainingsset ? - Wie sollte man mit ihnen umgehen?",
    "selectable": false
   },
   "source": [
    "* We decide to keep all the outliers, thea are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Vergleiche `'WheelTypeID'` mit `'WheelType'` und `'VehAge'` mit `'VehYear'`.",
    "hint_counter": 1,
    "original-content": "",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering, see the paper:\n",
    "\n",
    "https://www.kaggle.com/code/geovaniwoll/machine-learningproject\n",
    "\n",
    "We apply the idea with a new feature:\n",
    "\n",
    "# severity_index and phq_gad_relation \n",
    "df['severity_index'] = df['phq_score'] + df['gad_score']\n",
    "df['phq_gad_relation'] = df['phq_score'] / df['gad_score']\n",
    "\n",
    "# Polynomal features\n",
    "\n",
    "We apply a new target (target-engineering), see:\n",
    "\n",
    "df['treatment_status'] = df['depression_treatment'] | df['anxiety_treatment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\n",
    "features_train['severity_index'] = features_train['phq_score'] + features_train['gad_score']\n",
    "features_train['phq_gad_relation'] = features_train['phq_score'] / features_train['gad_score']\n",
    "\n",
    "# Part: new features\n",
    "\n",
    "features_train['depressiveness_awareness'] = features_train['depression_diagnosis'] | features_train['depression_treatment']\n",
    "features_train['anxiety_awareness'] = features_train['anxiety_diagnosis'] | features_train['anxiety_treatment']\n",
    "\n",
    "df_mod = features_train.copy()\n",
    "\n",
    "df_mod['depressiveness'] = target_train['depressiveness']\n",
    "\n",
    "display(df_mod.head())\n",
    "\n",
    "\n",
    "num_new_cols = ['severity_index', 'phq_gad_relation', 'depressiveness_awareness', 'anxiety_awareness']\n",
    "\n",
    "# Histogram and boxplot for the numerical features with the target \"depressiveness\" as Hue\n",
    "\n",
    "\n",
    "# Histogram and boxplot for the numerical features with the target \"depressiveness\" as Hue\n",
    "for col in num_new_cols:\n",
    "\n",
    "    # Full plot histogram and boxplot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 10), sharex=True, gridspec_kw={'height_ratios': [5, 1]})\n",
    "    \n",
    "    # Histogram\n",
    "    sns.histplot(data=df_mod, x=col, hue='depressiveness', kde=True, multiple=\"stack\", ax=axes[0])\n",
    "    axes[0].set_title(f'Histogram of {col} by depressiveness')\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df_mod, x=col, hue='depressiveness', ax=axes[1])\n",
    "    axes[1].set_title(f'Boxplot of {col} by depressiveness')\n",
    "\n",
    "    # Titles of the axis\n",
    "    axes[1].set_xlabel(col)\n",
    "    axes[1].set_ylabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, you can check just the new features for NaNs\n",
    "print(\"\\nNumber of NaN values in the newly engineered features:\")\n",
    "print(features_train[['severity_index', 'phq_gad_relation']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# Design new prosperous features\n",
    "\n",
    "\n",
    "\n",
    "def engineer_features(X, y=None, inplace=False):\n",
    "    if not inplace:\n",
    "        X = X.copy()\n",
    "        if y is not None:\n",
    "            y = y.copy()\n",
    "\n",
    "    X['severity_index'] = X['phq_score'] + X['gad_score']\n",
    "    X['phq_gad_relation'] = X['phq_score'] / X['gad_score']\n",
    "    \n",
    "    # Features: out together the diagnosis and treatment\n",
    "    X['depressiveness_awareness'] = X['depression_diagnosis'] | X['depression_treatment']\n",
    "    X['anxiety_awareness'] = X['anxiety_diagnosis'] | X['anxiety_treatment']\n",
    "\n",
    "    if y is not None:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Feature-Engineering auf train und test anwenden\n",
    "features_train, target_train = engineer_features(features_train, target_train)\n",
    "display(features_train)\n",
    "\n",
    "features_test, target_test = engineer_features(features_test, target_test)\n",
    "display(features_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new features\n",
    "\n",
    "display(features_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean with the outliers\n",
    "\n",
    "\n",
    "def sampling_features(X, y=None, inplace=False):\n",
    "    if not inplace:\n",
    "        X = X.copy()\n",
    "        if y is not None:\n",
    "            y = y.copy()\n",
    " \n",
    "    # Outliers, if necessary\n",
    "#    if y is not None:\n",
    "#        mask = X['epworth_score'] > 21\n",
    "#        X = X[mask]\n",
    "#        y = y[mask]\n",
    "#        return X, y\n",
    "#    else:\n",
    "#        X = X[X['epworth_score'] > 21]\n",
    "#        return X\n",
    "    \n",
    "    if y is not None:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Resample",
    "selectable": false
   },
   "source": [
    "### Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Wie du sicherlich festgestellt hast, sind die Zielkategorien im Datensatz sehr unausgeglichen. Es kann also notwendig sein, dein Trainingsdatenset zu resamplen. In *Unausgeglichene Zeilkategorien* (Modul 2, Kapitel 3) hast du verschiedene Methoden des *resampling* kennengelernt und auch gelernt, wie man `imblearn.pipeline` nutzt, um verschiedene Samplingmethoden zu testen.",
    "selectable": true
   },
   "source": [
    "The imbalanced target is *resampling*, e.g. `imblearn.pipeline` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also at the beginning:\n",
    "\n",
    "print(target_resampled)\n",
    "# Assuming target_resampled is a Pandas Series, check the balance\n",
    "print(target_resampled.value_counts())\n",
    "\n",
    "# Optionally, you can also print the proportion of each class\n",
    "print(target_resampled.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the files with pickle (optional)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# state = {\n",
    "#     'features_train': features_train,\n",
    "#     'target_train': target_train,\n",
    "#     'features_test': features_test,\n",
    "#     'target_test': target_test,    \n",
    "# }\n",
    "# # Dictionary speichern:\n",
    "# with open('state.pkl', 'wb') as f:\n",
    "#     pickle.dump(state, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Reopen the stored pickle-files (optional):\n",
    "# import pickle\n",
    "\n",
    "# with open('state.pkl', 'rb') as f:\n",
    "#     state = pickle.load(f)\n",
    "\n",
    "# features_train = state['features_train']\n",
    "# target_train = state['target_train']\n",
    "\n",
    "# features_test = state['features_test']\n",
    "# target_test = state['target_test']\n",
    "\n",
    "\n",
    "# #rf_pipe = state['rf_pipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "display(features_train)\n",
    "\n",
    "display(features_train.dtypes)\n",
    "\n",
    "display(target_train)\n",
    "\n",
    "display(target_train.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Modeling",
    "selectable": false
   },
   "source": [
    "## Modeling\n",
    "\n",
    "Here we apply the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features_train.head(2))\n",
    "display(features_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# define num_cols and cat_cols\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# define num_cols and cat_cols for the depressiveness\n",
    "\n",
    "# Important Columns and their relation, e.g., for the PCA\n",
    "\n",
    "#num_cols = ['school_year', 'age', 'gender', 'bmi', 'phq_score', 'depression_diagnosis', 'gad_score',\n",
    "#            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "\n",
    "num_new_features = ['severity_index', 'depressiveness_awareness', 'anxiety_awareness']  # delicate while NaN , 'phq_gad_relation']\n",
    "\n",
    "#num_cols_no_pca = ['gender', 'bmi']\n",
    "\n",
    "#num_cols_pca = ['school_year', 'age', 'depression_diagnosis', 'gad_score',\n",
    "#            'anxiety_diagnosis', 'epworth_score', 'sleepiness']\n",
    "\n",
    "targets = ['anxiousness', 'depressiveness', 'treatment_status', 'suicidal']\n",
    "\n",
    "# (depression_treatment and anxiety_treatment  are columns, which are used in the target treatment_status)\n",
    "\n",
    "#cat_cols = ['who_bmi', 'depression_severity', 'anxiety_severity']\n",
    "#cat_cols = ['who_bmi', 'anxiety_severity']\n",
    "cat_cols = []\n",
    "\n",
    "# num-cols for the target depressiveness\n",
    "\n",
    "num_cols = ['age', 'gender', 'gad_score', 'epworth_score', 'bmi', 'depressiveness_awareness', 'anxiety_awareness'] # , 'severity_index']\n",
    "\n",
    "#num_cols = [] \n",
    "num_cols_poly = ['age','gender', 'gad_score', 'epworth_score', 'bmi', 'depressiveness_awareness', 'anxiety_awareness']\n",
    "num_cols_poly = []\n",
    "target = ['depressiveness']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# instantiate model\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# build pipeline\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"target_enc\", ce.TargetEncoder())  # TargetEncoder hinzufügen\n",
    "#    (\"ohe\", OneHotEncoder(handle_unknown='ignore')),  # One-Hot encoder\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "#    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # PolynomialFeatures hinzufügen, bringt nichts, da man\n",
    "#    Lineare Kombinationen hat\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "num_pipe_poly = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # PolynomialFeatures hinzufügen, bringt nichts, da man\n",
    "#    Lineare Kombinationen hat\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# for the depressiveness as target, we only have num-cols\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_cols),\n",
    "#       (\"num_trans_poly\", num_pipe_poly, num_cols_poly),\n",
    "#        (\"cat_trans\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder = \"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "rf_pipe = ImbPipeline(steps=[\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers\n",
    "    (\"pre\", preprocessing),\n",
    "    (\"kbest\", SelectKBest(k=20, score_func=f_classif)),\n",
    "    (\"model\", RandomForestClassifier()),\n",
    "#    (\"model\", RandomForestClassifier_mod()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# fit pipeline on cleaned (and filtered) training set\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# fit pipeline on cleaned (and filtered) training set\n",
    "\n",
    "rf_pipe.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 0,
    "original-content": "# predict and evaluate on test set\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# predict and evaluate on test set\n",
    "\n",
    "print(classification_report(target_test, rf_pipe.predict(features_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#              precision    recall  f1-score   support\n",
    "#\n",
    "#           0       0.86      0.94      0.90        54\n",
    "#           1       0.81      0.62      0.70        21\n",
    "#\n",
    "#   accuracy                           0.85        75\n",
    "#   macro avg       0.84      0.78      0.80        75\n",
    "#weighted avg       0.85      0.85      0.85        75\n",
    "\n",
    "\n",
    "#polynomial feature all num. features\n",
    "#              precision    recall  f1-score   support\n",
    "#\n",
    "#           0       0.93      0.98      0.95        54\n",
    "#           1       0.94      0.81      0.87        21\n",
    "#\n",
    "#    accuracy                           0.93        75\n",
    "#   macro avg       0.94      0.90      0.91        75\n",
    "# weighted avg       0.93      0.93      0.93        75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature names after the transformation\n",
    "preprocessor = rf_pipe.named_steps['pre']\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Selecting the features after SelectKBest\n",
    "kbest = rf_pipe.named_steps['kbest']\n",
    "selected_feature_names = [feature_names[i] for i in kbest.get_support(indices=True)]\n",
    "\n",
    "# Extracting feature importances from the RandomForest model\n",
    "rf_model = rf_pipe.named_steps['model']\n",
    "feature_importance_rf = pd.Series(rf_model.feature_importances_, index=selected_feature_names)\n",
    "\n",
    "# Sorting the values and selecting the top 20 features\n",
    "top_20_features = feature_importance_rf.nlargest(20)\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.style.use('fivethirtyeight')\n",
    "top_20_features.plot(kind='barh', figsize=(10, 8))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in Random Forest')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Data Scaling",
    "selectable": false
   },
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Modelle performen in der Regel besser, wenn du die Daten vorher skalierst. Am besten nutzt du hierfür die *transformer* aus `sklearn.preprocessing`. Du kannst Sie hier instanziieren, testen und dann später in deine Pipeline einbauen.",
    "selectable": true
   },
   "source": [
    "This is done with the standardscalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Dimensionality Reduction",
    "selectable": false
   },
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Hast du in deiner EDA stark korellierte Features entdeckt oder einfach insgesamt zu viele Features? Dann empfehle ich dir, zu prüfen, ob Dimensionsreduzierung z.B. durch eine `PCA` sinnvoll ist. Wir haben das Vorgehen hierzu in *Modul 1, Kapitel 4* behandelt.",
    "selectable": true
   },
   "source": [
    "Here, we apply the knowledge of the previous EDA and PowerBI:\n",
    "\n",
    "    all organic  (age, gender, etc)\n",
    "\n",
    "    BMI and EpScore (OR the who_bmi and Sleepiness classification columns) \n",
    "\n",
    "    Anxiety (GAD score OR anxiety severity OR anxiousness) \n",
    "\n",
    "    Anxiety diagnosis OR treatment \n",
    "\n",
    "    Depression diagnosis OR treatment (since independent of the test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "original-content": "",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "target = ['depressiveness']\n",
    "\n",
    "\n",
    "# Korrelationsmatrix\n",
    "corr_matrix = features_train[num_cols].corr()\n",
    "\n",
    "# Heatmap der Korrelationsmatrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin = -1, vmax = 1, fmt='.2f')\n",
    "plt.title('Korrelationsmatrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Train model",
    "selectable": false
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_demo",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "original-content": "from sklearn.exceptions import DataConversionWarning\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_demo",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "original-content": "#useful imports\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, classification_report",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "#useful imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we built the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "#   (\"ohe\", OneHotEncoder(handle_unknown='ignore')),\n",
    "    (\"target_enc\", ce.TargetEncoder()),  # TargetEncoder hinzufügen    \n",
    "])\n",
    "\n",
    "# Macht hier mehr Sinn\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "#    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # PolynomialFeatures hinzufügen\n",
    "    (\"scaler\", StandardScaler()),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline für numerische Daten mit PCA\n",
    "num_pipe_pca = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "#    (\"pca\", PCA(n_components=0.90))  # 90% der Varianz beibehalten\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wähle zuerst deine Features aus. Zu Beginn kannst du auch alle Features nutzen. In späteren Schritten kann es aber sinnvoll sein, nur die relevanten Features auszuwählen. Falls du die Daten transformierst, empfiehlt es sich eine `Pipeline` zu verwenden.\n\nDie logistische Regression hat wenige Hyperparameter und eine geringe Rechenzeit. Für eine erste Einschätzung könntest du sie nutzen.",
    "original-content": "# build unoptimized model\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# RF model\n",
    "\n",
    "\n",
    "\n",
    "num_cols = ['age', 'gender', 'bmi', 'gad_score', 'epworth_score', 'depressiveness_awareness', 'anxiety_awareness']\n",
    "\n",
    "\n",
    "target = ['depressiveness']\n",
    "\n",
    "\n",
    "\n",
    "# We have only numerical cols\n",
    "\n",
    "preprocessing_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_cols),\n",
    "#        (\"num_pca\", num_pipe_pca, num_cols_pca),\n",
    "#        (\"cat_trans\", cat_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# PolynomialFeatures hinzufügen\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "rf_pipe_pca = ImbPipeline(steps=[\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers oder Rausnehmen!!! Aufpassen am Ende!!!\n",
    "    (\"pre\", preprocessing_rf),\n",
    "#    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "    (\"model\", RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(rf_pipe_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline on cleaned (and filtered) training set\n",
    "\n",
    "rf_pipe_pca.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(features_train)\n",
    "print(classification_report(target_test, rf_pipe_pca.predict(features_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the models:\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Manually close the current log handlers\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Ensure the 'log' directory exists\n",
    "log_dir = '../log'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Confirm that the directory was created\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"Directory '{log_dir}' exists or was successfully created.\")\n",
    "else:\n",
    "    print(f\"Failed to create directory '{log_dir}'.\")\n",
    "\n",
    "# Check the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Configure logging explicitly\n",
    "log_file_path = os.path.join(log_dir, 'best_model_evaluation.log')\n",
    "print(f\"Log file path: {log_file_path}\")  # This should point to the 'log' directory\n",
    "\n",
    "# Create and configure the logger explicitly\n",
    "logger = logging.getLogger('model_evaluation_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Avoid duplicate logs by clearing existing handlers if any\n",
    "\n",
    "if logger.hasHandlers():\n",
    "\n",
    "    logger.handlers.clear()\n",
    "\n",
    "\n",
    "# Create a file handler for logging\n",
    "\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Create a logging format\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "# Add the file handler to the logger\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "# Example log entry\n",
    "\n",
    "logger.info(\"Logging to the log/best_model_evaluation.log file.\")\n",
    "\n",
    "print(\"Logging setup completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def log_model(model_pipeline, features_test, target_test):\n",
    "    \"\"\"\n",
    "    Logs the details of a model including its name, parameters, and classification report.\n",
    "\n",
    "    Args:\n",
    "    - model_pipeline: The model pipeline to be logged.\n",
    "    - features_test: The test features used to generate the classification report.\n",
    "    - target_test: The test target used to generate the classification report.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model name\n",
    "    model_name = type(model_pipeline.named_steps['model']).__name__\n",
    "\n",
    "    # Model parameters\n",
    "    model_params = model_pipeline.named_steps['model'].get_params()\n",
    "\n",
    "    # Classification report\n",
    "    classification_rep = classification_report(target_test, model_pipeline.predict(features_test))\n",
    "\n",
    "    # Log the model details\n",
    "    logger.info(\"Model Evaluation\")\n",
    "    logger.info(f\"Model Name: {model_name}\")\n",
    "    logger.info(\"Model Parameters:\")\n",
    "    for param, value in model_params.items():\n",
    "        logging.info(f\"{param}: {value}\")\n",
    "\n",
    "    # Log the classification report\n",
    "    logger.info(\"Classification Report:\")\n",
    "    logger.info(\"\\n\" + classification_rep)\n",
    "\n",
    "    print(f\"Model details and evaluation have been logged to 'log/best_model_evaluation.log'.\")\n",
    "\n",
    "# Example usage\n",
    "log_model(rf_pipe_pca, features_test, target_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 2:\n",
    "# logistic regression\n",
    "\n",
    "\n",
    "preprocessing_log = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_cols),\n",
    "#        (\"num_pca\", num_pipe_pca, num_cols_pca),\n",
    "#        (\"cat_trans\", cat_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# PolynomialFeatures hinzufügen\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "log_pipe_pca = ImbPipeline(steps=[\n",
    "#    (\"pre\", preprocessing_rf),\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers oder Rausnehmen!!! Aufpassen am Ende!!!\n",
    "    (\"pre\", preprocessing_log),\n",
    "#    (\"poly\", poly_features),   # später ausprobieren, bringt es nicht mehr\n",
    "#    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "#    (\"model\", LogisticRegression(max_iter=1000))\n",
    "    (\"model\", LogisticRegression(max_iter=5000))    \n",
    "])\n",
    "\n",
    "\n",
    "print(log_pipe_pca)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline on cleaned (and filtered) training set\n",
    "\n",
    "log_pipe_pca.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate on test set\n",
    "# Auf dem Testset die Feature-Engineering anwenden)\n",
    "#\n",
    "#display(target_test)\n",
    "#display(features_test)\n",
    "\n",
    "#display(features_train)\n",
    "print(classification_report(target_test, log_pipe_pca.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the output \n",
    "log_model(log_pipe_pca, features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: KNN\n",
    "\n",
    "\n",
    "preprocessing_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_cols),\n",
    "#        (\"num_pca\", num_pipe_pca, num_cols_pca),\n",
    "#        (\"cat_trans\", cat_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# PolynomialFeatures hinzufügen\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "knn_pipe_pca = ImbPipeline(steps=[\n",
    "#    (\"pre\", preprocessing_rf),\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers oder Rausnehmen!!! Aufpassen am Ende!!!\n",
    "    (\"pre\", preprocessing_knn),\n",
    "#    (\"poly\", poly_features),   # später ausprobieren, bringt es nicht mehr\n",
    "#    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "print(knn_pipe_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it\n",
    "\n",
    "knn_pipe_pca.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(features_train)\n",
    "print(classification_report(target_test, knn_pipe_pca.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the output \n",
    "log_model(knn_pipe_pca, features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Decision Tree\n",
    "\n",
    "\n",
    "preprocessing_dec = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_trans\", num_pipe, num_cols),\n",
    "#        (\"num_pca\", num_pipe_pca, num_cols_pca),\n",
    "#        (\"cat_trans\", cat_pipe, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# PolynomialFeatures hinzufügen\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "\n",
    "dec_pipe_pca = ImbPipeline(steps=[\n",
    "#    (\"pre\", preprocessing_rf),\n",
    "    (\"oversampler\", RandomOverSampler()),   # Hinzufügen des RandomOverSamplers oder Rausnehmen!!! Aufpassen am Ende!!!\n",
    "    (\"pre\", preprocessing_dec),\n",
    "#    (\"poly\", poly_features),   # später ausprobieren, bringt es nicht mehr\n",
    "#    (\"kbest\", SelectKBest(k=10, score_func=f_classif)),\n",
    "    (\"model\", DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "print(dec_pipe_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline on cleaned (and filtered) training set\n",
    "\n",
    "dec_pipe_pca.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(features_train)\n",
    "print(classification_report(target_test, dec_pipe_pca.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the output \n",
    "log_model(dec_pipe_pca, features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Curve zum Sichtbarmachung der Lernens\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "# Definiere die Tiefe des Baums, die getestet werden soll\n",
    "depth = [3, 6, 9, 10, 11, 12] # , 13, 14, 15, 18, 21]\n",
    "\n",
    "# Berechne die Validierungskurve\n",
    "train_scores, test_scores = validation_curve(\n",
    "    estimator=dec_pipe_pca, # Die Pipeline\n",
    "    X=features_train, # Merkmalsmatrix\n",
    "    y=target_train, # Zielvektor\n",
    "    param_name='model__max_depth', # Hyperparameter des DecisionTreeClassifiers innerhalb der Pipeline\n",
    "    param_range=depth, # Werte des Hyperparameters\n",
    "    cv=5, # 5-fache Kreuzvalidierung\n",
    "    scoring='f1', # F1-Score zur Bewertung\n",
    "    n_jobs=-1 # Alle verfügbaren Kerne verwenden, um zu beschleunigen\n",
    ")\n",
    "\n",
    "# Mittelwert und Standardabweichung der Trainings- und Test-Scores berechnen\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Visualisiere die Validierungskurve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Validation Curve with Decision Tree')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "\n",
    "# Fehlerbalken für Trainingsscores\n",
    "plt.plot(depth, train_scores_mean, label='Training score', color='r')\n",
    "plt.fill_between(depth, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='r')\n",
    "\n",
    "# Fehlerbalken für Testscores\n",
    "plt.plot(depth, test_scores_mean, label='Cross-validation score', color='g')\n",
    "plt.fill_between(depth, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color='g')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# F1-Scores in einem DataFrame anzeigen\n",
    "F1 = pd.DataFrame({\n",
    "    'max_depth': depth,\n",
    "    'Training F1-score': train_scores_mean,\n",
    "    'Validation F1-score': test_scores_mean\n",
    "})\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Klassifikationsberichte erstellen\n",
    "dec_report = classification_report(target_test, dec_pipe_pca.predict(features_test), output_dict=True)\n",
    "knn_report = classification_report(target_test, knn_pipe_pca.predict(features_test), output_dict=True)\n",
    "log_report = classification_report(target_test, log_pipe_pca.predict(features_test), output_dict=True)\n",
    "rf_report = classification_report(target_test, rf_pipe_pca.predict(features_test), output_dict=True)\n",
    "\n",
    "# Funktion zum Extrahieren der Metriken aus dem Bericht\n",
    "def extract_metrics(report, model_name):\n",
    "    metrics = []\n",
    "    for key in ['0', '1', 'weighted avg']:\n",
    "        precision = report[key]['precision']\n",
    "        recall = report[key]['recall']\n",
    "        f1_score = report[key]['f1-score']\n",
    "        metrics.append({\n",
    "            'Model': model_name,\n",
    "            'Class': key,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_score\n",
    "        })\n",
    "    return metrics\n",
    "\n",
    "# Extrahiere die Metriken für alle Modelle\n",
    "all_metrics = []\n",
    "all_metrics.extend(extract_metrics(dec_report, 'Decision Tree'))\n",
    "all_metrics.extend(extract_metrics(knn_report, 'KNN'))\n",
    "all_metrics.extend(extract_metrics(log_report, 'Logistic Regression'))\n",
    "all_metrics.extend(extract_metrics(rf_report, 'Random Forest'))\n",
    "\n",
    "# Erstelle einen DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Ausgabe der Tabelle\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging the results\n",
    "\n",
    "\n",
    "# Log the metrics for a single model\n",
    "def log_metrics(metrics):\n",
    "    for metric in metrics:\n",
    "        logger.info(f\"Model: {metric['Model']}, Class: {metric['Class']}, Precision: {metric['Precision']:.4f}, Recall: {metric['Recall']:.4f}, F1 Score: {metric['F1 Score']:.4f}\")\n",
    "\n",
    "\n",
    "# Log all metrics\n",
    "log_metrics(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finde das Modell mit dem höchsten F1-Score für die Klasse '1'\n",
    "best_model_row = metrics_df[metrics_df['Class'] == '1'].sort_values(by='F1 Score', ascending=False).iloc[0]\n",
    "best_model_name = best_model_row['Model']\n",
    "\n",
    "# Abspeichern des besten Modells\n",
    "if best_model_name == 'Decision Tree':\n",
    "    best_model = dec_pipe_pca\n",
    "elif best_model_name == 'KNN':\n",
    "    best_model = knn_pipe_pca\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_model = log_pipe_pca\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_pipe_pca\n",
    "\n",
    "print(f\"The best model is: {best_model_name} with F1 Score: {best_model_row['F1 Score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\".....\")\n",
    "logger.info(f\"The best model is: {best_model_name} with F1 Score: {best_model_row['F1 Score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimation of the logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter Grid definieren\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],  # Regularisierungsparameter\n",
    "    'model__solver': ['newton-cg', 'lbfgs', 'sag'],\n",
    "#    'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # wäre noch bei L1 angesagt\n",
    "    'model__penalty': ['l2'],  # L2-Regularisierung\n",
    "}\n",
    "\n",
    "# Logistic Regression Pipeline\n",
    "log_pipe_pca = ImbPipeline(steps=[\n",
    "    (\"oversampler\", RandomOverSampler()),  # Hinzufügen des RandomOverSamplers\n",
    "    (\"pre\", preprocessing_log),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))  # Erhöhen der maximalen Iterationen\n",
    "])\n",
    "\n",
    "# GridSearchCV durchführen\n",
    "\n",
    "# 'recall' ev. dann optimiert man den Recall!\n",
    "grid_search_log = GridSearchCV(log_pipe_pca, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "#grid_search_log = GridSearchCV(log_pipe_pca, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "grid_search_log.fit(features_train, target_train)\n",
    "\n",
    "# Beste Parameter und Ergebnis anzeigen\n",
    "print(f'Best parameters: {grid_search_log.best_params_}')\n",
    "print(f'Best score: {grid_search_log.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\".....\")\n",
    "logger.info(\"The best model is: logistic regression method is:\")\n",
    "logger.info(f'Best parameters: {grid_search_log.best_params_}')\n",
    "logger.info(f'Best score: {grid_search_log.best_score_}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Save the best model\n",
    "best_model_log = grid_search_log.best_estimator_\n",
    "\n",
    "print(f'The best model in logistic regression is {best_model_log}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the best log.-Regression Modell\n",
    "print(classification_report(target_test, grid_search_log.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Nutze dazu z.B. `GridSearchCV`. Verwende zuerst einen kleinen Suchraum, um Rechenzeit zu sparen. Anschließend kannst du ihn verfeinern.",
    "original-content": "# tune hyperparameters\n",
    "selectable": true
   },
   "outputs": [],
   "source": [
    "# Optimation of Random-forest\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Parameter Grid definieren\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # , 300],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__max_depth': [10, 20],  # , 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [2, 4],  # [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV durchführen # opimise the model after precision\n",
    "grid_search_rf = GridSearchCV(rf_pipe_pca, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "#grid_search_rf = GridSearchCV(rf_pipe_pca, param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "grid_search_rf.fit(features_train, target_train)\n",
    "\n",
    "# Beste Parameter und Ergebnis anzeigen\n",
    "print(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "print(f'Best score: {grid_search_rf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the best RF model\n",
    "print(classification_report(target_test, grid_search_rf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save and print the best model\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(f'The best model in Random Forest is {best_model_rf}.')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\".....\")\n",
    "logger.info(\"The best model is: Random Forest method is:\")\n",
    "logger.info(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "logger.info(f'Best score: {grid_search_rf.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimation of KNN\n",
    "\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid_knn = {\n",
    "    'model__n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider\n",
    "    'model__weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'model__metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metric to use\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for the KNN pipeline\n",
    "grid_search_knn = GridSearchCV(knn_pipe_pca, param_grid_knn, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search_knn.fit(features_train, target_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(f'Best parameters for KNN: {grid_search_knn.best_params_}')\n",
    "print(f'Best score for KNN: {grid_search_knn.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clssification-report of the KNN model\n",
    "\n",
    "print(classification_report(target_test, grid_search_knn.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\".....\")\n",
    "logger.info(\"The best model is: KNN method is:\")\n",
    "logger.info(f'Best parameters: {grid_search_knn.best_params_}')\n",
    "logger.info(f'Best score: {grid_search_knn.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "### Model selection",
    "selectable": false
   },
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Wähle das beste Modell aus. Entscheide dabei selbst, welche Metrik dir am wichtigsten ist. Mit `confusion_matrix()` aus `sklearn.metrics` kannst du genau sehen, wie viele Datenpunkte jeweils richtig und falsch klassifiziert wurden. Vielleicht hilft dir das bei deiner Entscheidung.",
    "selectable": true
   },
   "source": [
    "Choose the best model. Decide for yourself which metric is most important to you. Using `confusion_matrix()` from `sklearn.metrics` you can see exactly how many data points were classified correctly and incorrectly. Maybe this will help you with your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Entscheide dich für dein Modell, welches du für die Vorhersage der Test- und Zieldaten nutzen möchtest.",
    "hint_counter": 1,
    "original-content": "# select model\n",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Machen Sie Vorhersagen auf dem Testdatensatz\n",
    "predictions = best_model_rf.predict(features_test)\n",
    "\n",
    "# Berechnen Sie die Confusion Matrix\n",
    "cm = confusion_matrix(target_test, predictions)\n",
    "\n",
    "# Anzeigen der Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix of Random-Forest')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Log Regression\n",
    "\n",
    "# Machen Sie Vorhersagen auf dem Testdatensatz\n",
    "predictions = best_model_log.predict(features_test)\n",
    "\n",
    "# Berechnen Sie die Confusion Matrix\n",
    "cm = confusion_matrix(target_test, predictions)\n",
    "\n",
    "# Anzeigen der Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix of logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `best_model_rf` is your trained RandomForestClassifier and `features_test` is your test dataset\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "probabilities = best_model_rf.predict_proba(features_test)\n",
    "\n",
    "# Extract the probabilities for class 1 (second column in the array)\n",
    "prob_class_1 = probabilities[:, 1]  # Probability of class 1 for each prediction\n",
    "\n",
    "prob_class_0 = probabilities[:, 0]  # Probability of class 1 for each prediction\n",
    "\n",
    "\n",
    "\n",
    "# If you want to print the predictions along with their probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(features_test)):\n",
    "\n",
    "    # Get the predicted class for this sample\n",
    "\n",
    "\n",
    "    predicted_class = best_model_rf.predict(features_test.iloc[[i]])[0]\n",
    "    \n",
    "    if predicted_class == 0:\n",
    "        # Print the predicted class and the corresponding probability for class 1\n",
    "        print(f\"Sample {i+1}: Predicted Depressiveness = {predicted_class}, Probability = {prob_class_0[i]:.2f}\")\n",
    "    else:\n",
    "        print(f\"Sample {i+1}: Predicted Depressiveness = {predicted_class}, Probability = {prob_class_1[i]:.2f}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decide us for the RF-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Die finale Datenpipeline\n\nDu hast die Daten bereinigt, aufbereitet und ein Modell darauf trainiert. Kombiniere nun die jeweiligen Schritte in einer Funktion oder einer Pipeline, die ein Datenset einliest und Vorhersagen dafür erzeugt. Lösche keine Datenpunkte aus den Testdaten. Das könnte die Abschätzung der Vorhersagegüte, die das Modell im Einsatz hätte, verfälschen.",
    "selectable": false
   },
   "source": [
    "## Final Data-pipeline\n",
    "\n",
    "We appy the full data set to our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(csv_file, model):\n",
    "    \"\"\"Estimate waiting time in seconds for taxi-rides in the Quito area.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path of the incoming data csv file \n",
    "        model (model): The trained model.\n",
    "    \n",
    "    Returns:\n",
    "        numpyArray: Estimated waiting time [seconds]\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    test = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "    # Clean te file\n",
    "    test = clean_data(test)\n",
    "\n",
    "    # sample the file\n",
    "    test = sampling_features(test)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    test = engineer_features(test)\n",
    "\n",
    "    # Prediction\n",
    "    predictions = model.predict(test)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example to test the function is based on the stored features_test.csv, which is never toughed in the process and is used as a test\n",
    "csv_file = '../data/features_test.csv'\n",
    "\n",
    "# We decide us for the Random Forest as best modell\n",
    "\n",
    "#best_pipe = dec_pipe_pca  \n",
    "# best_pipe = best_model_log \n",
    "best_pipe = best_model_rf \n",
    "#best_pipe = best_model_rf\n",
    "#best_pipe = best_model_log  \n",
    "\n",
    "\n",
    "trained_model = best_pipe # chossing the best model\n",
    "test = predict(csv_file, trained_model)\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 1,
    "original-content": "",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is an alternative of the prediction-function, if we assume also to store the target, while we use it in the clean_data, sampling_features and engineering_features\n",
    "\n",
    "# def predict(csv_file_1, csv_file_2, model):\n",
    "#     \"\"\"Estimate waiting time in seconds for taxi-rides in the Quito area.\n",
    "    \n",
    "#     Args:\n",
    "#         csv_file (str): Path of the incoming data csv file \n",
    "#         model (model): The trained model.\n",
    "    \n",
    "#     Returns:\n",
    "#         numpyArray: Estimated waiting time [seconds]\n",
    "#     \"\"\"\n",
    "#     # Read data\n",
    "#     test = pd.read_csv(csv_file_1)\n",
    "#     target = pd.read_csv(csv_file_2)\n",
    "\n",
    "\n",
    "#     # clean data\n",
    "#     test, target = clean_data(test, target)\n",
    "\n",
    "#     # apply sampling\n",
    "#     test, target = sampling_features(test, target)\n",
    "    \n",
    "#     # Feature Engineering\n",
    "#     test, target = engineer_features(test, target)\n",
    "\n",
    "#     # Prediction\n",
    "#     predictions = model.predict(test)\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# csv_file_1 = 'features_test.csv'\n",
    "# csv_file_2 = 'target_test.csv'\n",
    "\n",
    "\n",
    "\n",
    "# best_pipe = best_model_rf  \n",
    "# #best_pipe = best_model_rf\n",
    "# #best_pipe = best_model_log  \n",
    "\n",
    "\n",
    "# trained_model = best_pipe \n",
    "# test = predict(csv_file_1, csv_file_2, trained_model)\n",
    "# print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification of the test-dates\n",
    "\n",
    "\n",
    "print(classification_report(target_test, predict(csv_file, trained_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Model Interpretation",
    "selectable": false
   },
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Um den Einkäufern des Unternehmens dein Modell schmackhaft zu machen, solltest du ihnen erklären können, welche Eigenschaften für das Modell von Bedeutung sind.\n\nWelche Features sind laut deinem Modell für die Vorhersage am wichtigsten?",
    "selectable": false
   },
   "source": [
    "We discuss the feature importance of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We apply the best trained model, e.g., best_model_rf\n",
    "trained_model = best_model_rf \n",
    "# trained_model = best_model_log\n",
    "\n",
    "# Calculate the original accuracy with the F1-Score\n",
    "y_pred = trained_model.predict(features_test)\n",
    "f1_orig = f1_score(target_test, y_pred)\n",
    "\n",
    "# Calculate the Permutation Feature Importance\n",
    "perm_importances = []\n",
    "\n",
    "for col in features_test.columns:\n",
    "    permuted_features = features_test.copy()\n",
    "    permuted_features[col] = np.random.permutation(permuted_features[col])\n",
    "    permuted_pred = trained_model.predict(permuted_features)\n",
    "    permuted_f1 = f1_score(target_test, permuted_pred)\n",
    "    perm_important = f1_orig - permuted_f1\n",
    "    perm_importances.append(perm_important)\n",
    "\n",
    "# Set a series with the Permutations importances and the column-names as index\n",
    "perm_importance = pd.Series(perm_importances, index=features_test.columns)\n",
    "\n",
    "# Sort for the top 10 features\n",
    "top_10_features = perm_importance.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Visualion of the Top 10 Permutationsimportances\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_10_features.plot(kind='barh')\n",
    "plt.xlabel('Feature Importance (F1 Score Decrease)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 10 Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert the series to get the importantest at the beginning\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "# Calculate the Permutation Feature Importance\n",
    "\n",
    "# features_train, target_train    Train-data-set\n",
    "# features_test, target_test      Test-data-set\n",
    "\n",
    "trained_model = best_model_rf \n",
    "#trained_model = best_model_log\n",
    "\n",
    "# Calculation of the original accuracy with the F1-Score\n",
    "y_pred = trained_model.predict(features_test)\n",
    "f1_orig = f1_score(target_test, y_pred)\n",
    "\n",
    "# Columns to focus on\n",
    "columns_finals = ['age', 'gender', 'bmi', 'gad_score', 'epworth_score', 'depressiveness_awareness', 'anxiety_awareness']\n",
    "\n",
    "# Computation of the Permutation Feature Importance\n",
    "perm_importances = []\n",
    "\n",
    "for col in columns_finals:\n",
    "    permuted_features = features_test.copy()\n",
    "    permuted_features[col] = np.random.permutation(permuted_features[col])\n",
    "    permuted_pred = trained_model.predict(permuted_features)\n",
    "    permuted_f1 = f1_score(target_test, permuted_pred)\n",
    "    perm_important = f1_orig - permuted_f1\n",
    "    perm_importances.append(perm_important)\n",
    "\n",
    "# Set a series of the permutation-importances and their column-name as index\n",
    "perm_importance = pd.Series(perm_importances, index=columns_finals)\n",
    "\n",
    "# Sorting the values\n",
    "perm_importance = perm_importance.sort_values()\n",
    "\n",
    "# Visualization of the Permutation Importances\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the features\n",
    "perm_importance.plot(kind='barh', figsize=(10, 8))\n",
    "plt.xlabel('Feature Importance (F1 Score Decrease)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Permutation Feature Importance for Selected Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "Wie ändert sich deine durchschnittliche Vorhersage, wenn du den Meilenstand `'VehOdo'` bzw. das Alter `'VehicleAge'` variierst. Würdest du das so erwarten?",
    "selectable": true
   },
   "source": [
    "All the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_train.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_default",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "## Abschluss des Projekts",
    "selectable": false
   },
   "source": [
    "## Final fitting of the best model with all the full dataset, here we use the train and test dataset for the fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_content_type": "markdown_selectable",
    "changed": false,
    "deletable": false,
    "editable": false,
    "original-content": "In der Datei *features_aim.csv* findest du die Features für deine abschließende Vorhersage.\nSpeichere nur deine vorhergesagten Werte für `'IsBadBuy'` in einer CSV-Datei namens \n`predictions_aim.csv`. Wie viele Käufe in den Zieldaten sollten laut deinem Modell lieber nicht getätigt werden?",
    "selectable": true
   },
   "source": [
    "We apply the file: data/depression_anxiety_data.csv, as a full dataset for fitting our best model and apply the dataset aim_test.csv for our prediction.\n",
    "We store the predicted values in predictions_depression.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fit(csv_file, model, target):\n",
    "    \"\"\" Final fitting of the best model\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path of the incoming data csv file.\n",
    "        model: The trained model.\n",
    "        target (list of str): List of target column names.\n",
    "    \n",
    "    Returns:\n",
    "        model: Fitted model.\n",
    "    \"\"\"\n",
    "    # Read the datafile\n",
    "    df_clean = pd.read_csv(csv_file)\n",
    "\n",
    "    # Skip the NaN of the Target  # Optional\n",
    "#    df_clean = clean_data_target(df_clean, target)\n",
    "    \n",
    "    # Separate features and target\n",
    "    features = df_clean.drop(columns=target)\n",
    "    target = df_clean[target]\n",
    "\n",
    "    # Optional additional cleaning if needed\n",
    "    features, target = clean_data(features, target)\n",
    "\n",
    "    # Apply sampling\n",
    "    features, target = sampling_features(features, target)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    features, target = engineer_features(features, target)\n",
    "\n",
    "    # Fit the model\n",
    "    fitted_model = model.fit(features, target)\n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "# Original and full dataset for the final training\n",
    "\n",
    "csv_file = '../data/depression_anxiety_data.csv'\n",
    "target = ['depressiveness']\n",
    "trained_model = best_pipe  # Best model (assumed to be defined elsewhere)\n",
    "\n",
    "fitted_model = fit(csv_file, trained_model, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_content_type": "code_user",
    "changed": false,
    "deletable": false,
    "editable": true,
    "hint": "Wenn du willst, kannst du diese Codezelle gern im Forum unter *Abschlussprojekt: Gebrauchtwagenkäufe* diskutieren.",
    "hint_counter": 1,
    "original-content": "",
    "selectable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Final prediction\n",
    "csv_file = '../data/aim_test.csv'\n",
    "trained_model = fitted_model # Bestes Modell\n",
    "\n",
    "# Calculate the prediction\n",
    "predictions = predict(csv_file, trained_model)\n",
    "\n",
    "# Number of predicted persons, e.g., depressivness\n",
    "num_predict = sum(predictions)\n",
    "\n",
    "# Vorhersagen in einer CSV-Datei speichern\n",
    "predictions_df = pd.DataFrame({'Depressiveness': predictions})\n",
    "predictions_df.to_csv('../Data/predictions_depression.csv', index=False)\n",
    "\n",
    "# Number of the depressions\n",
    "print(f\"Number of Depessiveness: {num_predict}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best-model, features and targets as a pickle-file for the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#  Save with pickle\n",
    "\n",
    "target_col = ['depressiveness']\n",
    "\n",
    "\n",
    "\n",
    "all_features = ['age', 'gender', 'bmi', 'gad_score', 'epworth_score', 'depressiveness_awareness', 'anxiety_awareness']\n",
    "\n",
    "with open('../models/best_model_depression.pkl', 'wb') as file:\n",
    "    pickle.dump(trained_model, file)\n",
    "    pickle.dump(target_col, file)\n",
    "    pickle.dump(all_features, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "content_id": "5546f3fd-971f-4387-824c-375dfcedfa49",
  "content_language": "de",
  "content_title": "Abschlussprojekt: Gebrauchtwagenkäufe",
  "content_type": "exercise",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
